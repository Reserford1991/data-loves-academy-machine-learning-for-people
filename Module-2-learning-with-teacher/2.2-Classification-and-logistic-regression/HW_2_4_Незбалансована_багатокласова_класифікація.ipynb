{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "У цьому ДЗ ми потренуємось розв'язувати задачу багатокласової класифікації за допомогою логістичної регресії з використанням стратегій One-vs-Rest та One-vs-One, оцінити якість моделей та порівняти стратегії."
   ],
   "metadata": {
    "id": "VUPArbcFJKzJ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Опис задачі і даних\n",
    "\n",
    "**Контекст**\n",
    "\n",
    "В цьому ДЗ ми працюємо з даними про сегментацію клієнтів.\n",
    "\n",
    "Сегментація клієнтів – це практика поділу бази клієнтів на групи індивідів, які схожі між собою за певними критеріями, що мають значення для маркетингу, такими як вік, стать, інтереси та звички у витратах.\n",
    "\n",
    "Компанії, які використовують сегментацію клієнтів, виходять з того, що кожен клієнт є унікальним і що їхні маркетингові зусилля будуть більш ефективними, якщо вони орієнтуватимуться на конкретні, менші групи зі зверненнями, які ці споживачі вважатимуть доречними та які спонукатимуть їх до купівлі. Компанії також сподіваються отримати глибше розуміння уподобань та потреб своїх клієнтів з метою виявлення того, що кожен сегмент цінує найбільше, щоб точніше адаптувати маркетингові матеріали до цього сегменту.\n",
    "\n",
    "**Зміст**.\n",
    "\n",
    "Автомобільна компанія планує вийти на нові ринки зі своїми існуючими продуктами (P1, P2, P3, P4 і P5). Після інтенсивного маркетингового дослідження вони дійшли висновку, що поведінка нового ринку схожа на їхній існуючий ринок.\n",
    "\n",
    "На своєму існуючому ринку команда з продажу класифікувала всіх клієнтів на 4 сегменти (A, B, C, D). Потім вони здійснювали сегментовані звернення та комунікацію з різними сегментами клієнтів. Ця стратегія працювала для них надзвичайно добре. Вони планують використати ту саму стратегію на нових ринках і визначили 2627 нових потенційних клієнтів.\n",
    "\n",
    "Ви маєте допомогти менеджеру передбачити правильну групу для нових клієнтів.\n",
    "\n",
    "В цьому ДЗ використовуємо дані `customer_segmentation_train.csv`[скачати дані](https://drive.google.com/file/d/1VU1y2EwaHkVfr5RZ1U4MPWjeflAusK3w/view?usp=sharing). Це `train.csv`з цього [змагання](https://www.kaggle.com/datasets/abisheksudarshan/customer-segmentation/data?select=train.csv)"
   ],
   "metadata": {
    "id": "7f4tzX6YomVv"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Завдання 1.** Завантажте та підготуйте датасет до аналізу. Виконайте обробку пропущених значень та необхідне кодування категоріальних ознак. Розбийте на тренувальну і тестувальну вибірку, де в тесті 20%. Памʼятаємо, що весь препроцесинг ліпше все ж тренувати на тренувальній вибірці і на тестувальній лише використовувати вже натреновані трансформери.\n",
    "Але в даному випадку оскільки значень в категоріях небагато, можна зробити обробку і на оригінальних даних, а потім розбити - це простіше. Можна також реалізувати процесинг і тренування моделі з пайплайнами. Обирайте як вам зручніше."
   ],
   "metadata": {
    "id": "NZFXPKx1JX-3"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T17:40:49.909815Z",
     "start_time": "2025-02-02T17:40:49.905330Z"
    }
   },
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "id": "I-mwGqPS5GAT",
    "ExecuteTime": {
     "end_time": "2025-02-02T17:17:15.408125Z",
     "start_time": "2025-02-02T17:17:15.394296Z"
    }
   },
   "cell_type": "code",
   "source": [
    "customers_df = pd.read_csv('csv/customer_segmentation_train.csv')\n",
    "\n",
    "customers_df.info()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8068 entries, 0 to 8067\n",
      "Data columns (total 11 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   ID               8068 non-null   int64  \n",
      " 1   Gender           8068 non-null   object \n",
      " 2   Ever_Married     7928 non-null   object \n",
      " 3   Age              8068 non-null   int64  \n",
      " 4   Graduated        7990 non-null   object \n",
      " 5   Profession       7944 non-null   object \n",
      " 6   Work_Experience  7239 non-null   float64\n",
      " 7   Spending_Score   8068 non-null   object \n",
      " 8   Family_Size      7733 non-null   float64\n",
      " 9   Var_1            7992 non-null   object \n",
      " 10  Segmentation     8068 non-null   object \n",
      "dtypes: float64(2), int64(2), object(7)\n",
      "memory usage: 693.5+ KB\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T16:39:47.414705Z",
     "start_time": "2025-02-02T16:39:47.406124Z"
    }
   },
   "cell_type": "code",
   "source": "customers_df.head()",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       ID  Gender Ever_Married  Age Graduated     Profession  Work_Experience  \\\n",
       "0  462809    Male           No   22        No     Healthcare              1.0   \n",
       "1  462643  Female          Yes   38       Yes       Engineer              NaN   \n",
       "2  466315  Female          Yes   67       Yes       Engineer              1.0   \n",
       "3  461735    Male          Yes   67       Yes         Lawyer              0.0   \n",
       "4  462669  Female          Yes   40       Yes  Entertainment              NaN   \n",
       "\n",
       "  Spending_Score  Family_Size  Var_1 Segmentation  \n",
       "0            Low          4.0  Cat_4            D  \n",
       "1        Average          3.0  Cat_4            A  \n",
       "2            Low          1.0  Cat_6            B  \n",
       "3           High          2.0  Cat_6            B  \n",
       "4           High          6.0  Cat_6            A  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Ever_Married</th>\n",
       "      <th>Age</th>\n",
       "      <th>Graduated</th>\n",
       "      <th>Profession</th>\n",
       "      <th>Work_Experience</th>\n",
       "      <th>Spending_Score</th>\n",
       "      <th>Family_Size</th>\n",
       "      <th>Var_1</th>\n",
       "      <th>Segmentation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>462809</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>22</td>\n",
       "      <td>No</td>\n",
       "      <td>Healthcare</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Low</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Cat_4</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>462643</td>\n",
       "      <td>Female</td>\n",
       "      <td>Yes</td>\n",
       "      <td>38</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Engineer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Average</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Cat_4</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>466315</td>\n",
       "      <td>Female</td>\n",
       "      <td>Yes</td>\n",
       "      <td>67</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Engineer</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Low</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Cat_6</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>461735</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>67</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Lawyer</td>\n",
       "      <td>0.0</td>\n",
       "      <td>High</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Cat_6</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>462669</td>\n",
       "      <td>Female</td>\n",
       "      <td>Yes</td>\n",
       "      <td>40</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>NaN</td>\n",
       "      <td>High</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Cat_6</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T17:13:35.326340Z",
     "start_time": "2025-02-02T17:13:35.322071Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print('Null values in Ever_Married column: ', customers_df.Ever_Married.isnull().sum())\n",
    "customers_df.Ever_Married.value_counts()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null values in Ever_Married column:  140\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Ever_Married\n",
       "Yes    4643\n",
       "No     3285\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T16:49:11.997744Z",
     "start_time": "2025-02-02T16:49:11.993219Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Нульові значення в колонці Profession:  124\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Profession\n",
       "Artist           2516\n",
       "Healthcare       1332\n",
       "Entertainment     949\n",
       "Engineer          699\n",
       "Doctor            688\n",
       "Lawyer            623\n",
       "Executive         599\n",
       "Marketing         292\n",
       "Homemaker         246\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6,
   "source": [
    "print('Null values in Profession column: ', customers_df.Profession.isnull().sum())\n",
    "customers_df.Profession.value_counts()"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T16:51:18.973107Z",
     "start_time": "2025-02-02T16:51:18.968613Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print('Number of null values in Work_Experience column: ', customers_df.Work_Experience.isnull().sum())\n",
    "customers_df.Work_Experience.value_counts()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of null values in Work_Experience column:  829\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Work_Experience\n",
       "1.0     2354\n",
       "0.0     2318\n",
       "9.0      474\n",
       "8.0      463\n",
       "2.0      286\n",
       "3.0      255\n",
       "4.0      253\n",
       "6.0      204\n",
       "7.0      196\n",
       "5.0      194\n",
       "10.0      53\n",
       "11.0      50\n",
       "12.0      48\n",
       "13.0      46\n",
       "14.0      45\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T16:53:36.610375Z",
     "start_time": "2025-02-02T16:53:36.605492Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print('Null values in Family_Size column: ', customers_df.Family_Size.isnull().sum())\n",
    "customers_df.Family_Size.value_counts()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null values in Family_Size column:  335\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Family_Size\n",
       "2.0    2390\n",
       "3.0    1497\n",
       "1.0    1453\n",
       "4.0    1379\n",
       "5.0     612\n",
       "6.0     212\n",
       "7.0      96\n",
       "8.0      50\n",
       "9.0      44\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T16:58:15.763141Z",
     "start_time": "2025-02-02T16:58:15.758931Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print('Null values in Var_1 column: ', customers_df.Var_1.isnull().sum())\n",
    "customers_df.Var_1.value_counts()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null values in Var_1 column:  76\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Var_1\n",
       "Cat_6    5238\n",
       "Cat_4    1089\n",
       "Cat_3     822\n",
       "Cat_2     422\n",
       "Cat_7     203\n",
       "Cat_1     133\n",
       "Cat_5      85\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T18:32:07.130223Z",
     "start_time": "2025-02-02T18:32:07.101571Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if 'ID' in customers_df.columns:\n",
    "    customers_df.drop('ID', axis=1, inplace=True)\n",
    "\n",
    "input_cols = list(customers_df.columns)[1:-1]\n",
    "target_col = 'Segmentation'\n",
    "\n",
    "inputs_df = customers_df[input_cols].copy()\n",
    "targets_df = customers_df[target_col].copy()\n",
    "\n",
    "numeric_cols = inputs_df.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_cols = inputs_df.select_dtypes('object').columns.tolist()\n",
    "\n",
    "numeric_imputer = SimpleImputer(strategy='mean')\n",
    "numeric_imputer.fit(inputs_df[numeric_cols])\n",
    "inputs_df[numeric_cols] = numeric_imputer.transform(inputs_df[numeric_cols])\n",
    "\n",
    "numeric_scaler = MinMaxScaler()\n",
    "numeric_scaler.fit(inputs_df[numeric_cols])\n",
    "inputs_df[numeric_cols] = numeric_scaler.transform(inputs_df[numeric_cols])\n",
    "\n",
    "# 1. Fit the categorical imputer and encoder on the categorical columns:\n",
    "categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "inputs_df[categorical_cols] = categorical_imputer.fit_transform(inputs_df[categorical_cols])\n",
    "\n",
    "categorical_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "encoded_array = categorical_encoder.fit_transform(inputs_df[categorical_cols])\n",
    "\n",
    "# 2. Create a new DataFrame from the encoded array, using the encoder’s feature names.\n",
    "encoded_cat_df = pd.DataFrame(encoded_array,\n",
    "                              columns=categorical_encoder.get_feature_names_out(categorical_cols),\n",
    "                              index=inputs_df.index)\n",
    "\n",
    "# 3. Drop the original categorical columns from inputs:\n",
    "inputs_df = inputs_df.drop(columns=categorical_cols)\n",
    "\n",
    "# 4. Concatenate the encoded categorical features with the remaining data:\n",
    "inputs_df = pd.concat([inputs_df, encoded_cat_df], axis=1)\n",
    "\n",
    "transformed_customers_df = pd.concat([inputs_df, targets_df], axis=1)\n",
    "\n",
    "transformed_customers_df.info()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8068 entries, 0 to 8067\n",
      "Data columns (total 27 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   Age                       8068 non-null   float64\n",
      " 1   Work_Experience           8068 non-null   float64\n",
      " 2   Family_Size               8068 non-null   float64\n",
      " 3   Ever_Married_No           8068 non-null   float64\n",
      " 4   Ever_Married_Yes          8068 non-null   float64\n",
      " 5   Graduated_No              8068 non-null   float64\n",
      " 6   Graduated_Yes             8068 non-null   float64\n",
      " 7   Profession_Artist         8068 non-null   float64\n",
      " 8   Profession_Doctor         8068 non-null   float64\n",
      " 9   Profession_Engineer       8068 non-null   float64\n",
      " 10  Profession_Entertainment  8068 non-null   float64\n",
      " 11  Profession_Executive      8068 non-null   float64\n",
      " 12  Profession_Healthcare     8068 non-null   float64\n",
      " 13  Profession_Homemaker      8068 non-null   float64\n",
      " 14  Profession_Lawyer         8068 non-null   float64\n",
      " 15  Profession_Marketing      8068 non-null   float64\n",
      " 16  Spending_Score_Average    8068 non-null   float64\n",
      " 17  Spending_Score_High       8068 non-null   float64\n",
      " 18  Spending_Score_Low        8068 non-null   float64\n",
      " 19  Var_1_Cat_1               8068 non-null   float64\n",
      " 20  Var_1_Cat_2               8068 non-null   float64\n",
      " 21  Var_1_Cat_3               8068 non-null   float64\n",
      " 22  Var_1_Cat_4               8068 non-null   float64\n",
      " 23  Var_1_Cat_5               8068 non-null   float64\n",
      " 24  Var_1_Cat_6               8068 non-null   float64\n",
      " 25  Var_1_Cat_7               8068 non-null   float64\n",
      " 26  Segmentation              8068 non-null   object \n",
      "dtypes: float64(26), object(1)\n",
      "memory usage: 1.7+ MB\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T18:37:48.471772Z",
     "start_time": "2025-02-02T18:37:48.464750Z"
    }
   },
   "cell_type": "code",
   "source": [
    "use_sample = False\n",
    "train_val_df, test_df = train_test_split(transformed_customers_df, test_size=0.2, random_state=42)\n",
    "train_df, val_df = train_test_split(transformed_customers_df, test_size=0.2, random_state=42)\n",
    "\n",
    "train_inputs = train_df.iloc[:, :-1].copy()\n",
    "train_targets = train_df.iloc[:, -1].copy()\n",
    "\n",
    "val_inputs = val_df.iloc[:, :-1].copy()\n",
    "val_targets = val_df.iloc[:, -1].copy()\n",
    "\n",
    "test_inputs = test_df.iloc[:, :-1].copy()\n",
    "test_targets = test_df.iloc[:, -1].copy()"
   ],
   "outputs": [],
   "execution_count": 48
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Завдання 2. Важливо уважно прочитати все формулювання цього завдання до кінця!**\n",
    "\n",
    "Застосуйте методи ресемплингу даних SMOTE та SMOTE-Tomek з бібліотеки imbalanced-learn до тренувальної вибірки. В результаті у Вас має вийти 2 тренувальних набори: з апсемплингом зі SMOTE, та з ресамплингом з SMOTE-Tomek.\n",
    "\n",
    "Увага! В нашому наборі даних є як категоріальні дані, так і звичайні числові. Базовий SMOTE не буде правильно працювати з категоріальними даними, але є його модифікація, яка буде. Тому в цього завдання є 2 виконання\n",
    "\n",
    "  1. Застосувати SMOTE базовий лише на НЕкатегоріальних ознаках.\n",
    "\n",
    "  2. Переглянути інформацію про метод [SMOTENC](https://imbalanced-learn.org/dev/references/generated/imblearn.over_sampling.SMOTENC.html#imblearn.over_sampling.SMOTENC) і використати цей метод в цій задачі. За цей спосіб буде +3 бали за це завдання і він рекомендований для виконання.\n",
    "\n",
    "  **Підказка**: аби скористатись SMOTENC треба створити змінну, яка містить індекси ознак, які є категоріальними (їх номер серед колонок) і передати при ініціації екземпляра класу `SMOTENC(..., categorical_features=cat_feature_indeces)`.\n",
    "  \n",
    "  Ви також можете розглянути варіант використання варіації SMOTE, який працює ЛИШЕ з категоріальними ознаками [SMOTEN](https://imbalanced-learn.org/dev/references/generated/imblearn.over_sampling.SMOTEN.html)"
   ],
   "metadata": {
    "id": "fhJzCBA7P0f8"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "# Perform random sampling\n",
    "smote = SMOTE(random_state=0)\n",
    "train_inputs_sm, train_targets_sm = smote.fit_resample(train_inputs, train_targets)\n",
    "\n",
    "smotetomek = SMOTETomek(random_state=0)\n",
    "train_inputs_smt, train_targets_smt = smotetomek.fit_resample(train_inputs, train_targets)"
   ],
   "metadata": {
    "id": "6NFUkQ_15HNX",
    "ExecuteTime": {
     "end_time": "2025-02-02T18:49:24.128564Z",
     "start_time": "2025-02-02T18:49:23.069799Z"
    }
   },
   "outputs": [],
   "execution_count": 49
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Завдання 3**.\n",
    "  1. Навчіть модель логістичної регресії з використанням стратегії One-vs-Rest з логістичною регресією на оригінальних даних, збалансованих з SMOTE, збалансованих з Smote-Tomek.  \n",
    "  2. Виміряйте якість кожної з натренованих моделей використовуючи `sklearn.metrics.classification_report`.\n",
    "  3. Напишіть, яку метрику ви обрали для порівняння моделей.\n",
    "  4. Яка модель найкраща?\n",
    "  5. Якщо немає суттєвої різниці між моделями - напишіть свою гіпотезу, чому?"
   ],
   "metadata": {
    "id": "ja4w_GgmT4D0"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "log_reg = LogisticRegression(solver='liblinear')\n",
    "ovr_model = OneVsRestClassifier(log_reg)\n",
    "\n",
    "ovr_model.fit(train_inputs, train_targets)\n",
    "ovr_predictions = ovr_model.predict(test_inputs)\n",
    "\n",
    "print('Original dataset \\n')\n",
    "print(classification_report(test_targets, ovr_predictions))\n",
    "\n",
    "log_reg = LogisticRegression(solver='liblinear')\n",
    "ovr_sm_model = OneVsRestClassifier(log_reg)\n",
    "\n",
    "ovr_sm_model.fit(train_inputs_sm, train_targets_sm)\n",
    "ovr_sm_predictions = ovr_sm_model.predict(test_inputs)\n",
    "\n",
    "print('SMOTE dataset \\n')\n",
    "print(classification_report(test_targets, ovr_sm_predictions))\n",
    "\n",
    "log_reg = LogisticRegression(solver='liblinear')\n",
    "ovr_smt_model = OneVsRestClassifier(log_reg)\n",
    "\n",
    "ovr_smt_model.fit(train_inputs_smt, train_targets_smt)\n",
    "ovr_smt_predictions = ovr_smt_model.predict(test_inputs)\n",
    "\n",
    "print('SMOTE-Tomek dataset \\n')\n",
    "print(classification_report(test_targets, ovr_smt_predictions))"
   ],
   "metadata": {
    "id": "nxWVeRan5JBh",
    "ExecuteTime": {
     "end_time": "2025-02-02T18:57:12.121564Z",
     "start_time": "2025-02-02T18:57:11.931724Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       0.40      0.49      0.44       391\n",
      "           B       0.36      0.13      0.19       369\n",
      "           C       0.46      0.59      0.52       380\n",
      "           D       0.66      0.70      0.68       474\n",
      "\n",
      "    accuracy                           0.49      1614\n",
      "   macro avg       0.47      0.48      0.46      1614\n",
      "weighted avg       0.48      0.49      0.47      1614\n",
      "\n",
      "SMOTE dataset \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       0.39      0.50      0.44       391\n",
      "           B       0.39      0.21      0.27       369\n",
      "           C       0.47      0.58      0.52       380\n",
      "           D       0.70      0.65      0.67       474\n",
      "\n",
      "    accuracy                           0.50      1614\n",
      "   macro avg       0.49      0.49      0.48      1614\n",
      "weighted avg       0.50      0.50      0.49      1614\n",
      "\n",
      "SMOTE-Tomek dataset \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       0.40      0.52      0.45       391\n",
      "           B       0.39      0.19      0.26       369\n",
      "           C       0.45      0.57      0.50       380\n",
      "           D       0.70      0.65      0.67       474\n",
      "\n",
      "    accuracy                           0.49      1614\n",
      "   macro avg       0.48      0.48      0.47      1614\n",
      "weighted avg       0.50      0.49      0.48      1614\n",
      "\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As we can see original dataset, SMOTE and SMOTE-Tomek datasets show similar metrics. It says that SMOTE and SMOTE-Tomek resampling did not give much improvement in my flow. The reason may be that we have not so imballanced classes. Maybe some more feature engineering may help to improve dataset results after resampling.\n",
    "As a conclusion, I can say that there is no difference which model to use in this particular case."
   ]
  }
 ]
}
