{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "**Секція 1. Логістична регресія з нуля.**\n",
    "\n",
    "Будемо крок за кроком будувати модель лог регресії з нуля для передбачення, чи буде врожай більше за 80 яблук (задача подібна до лекційної, але на класифікацію).\n",
    "\n",
    "Давайте нагадаємо основні формули для логістичної регресії.\n",
    "\n",
    "### Функція гіпотези - обчислення передбачення у логістичній регресії:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\sigma(x W^T + b) = \\frac{1}{1 + e^{-(x W^T + b)}}\n",
    "$$\n",
    "\n",
    "Де:\n",
    "- $ \\hat{y} $ — це ймовірність \"позитивного\" класу.\n",
    "- $ x $ — це вектор (або матриця для набору прикладів) вхідних даних.\n",
    "- $ W $ — це вектор (або матриця) вагових коефіцієнтів моделі.\n",
    "- $ b $ — це зміщення (bias).\n",
    "- $ \\sigma(z) $ — це сигмоїдна функція активації.\n",
    "\n",
    "### Як обчислюється сигмоїдна функція:\n",
    "\n",
    "Сигмоїдна функція $ \\sigma(z) $ має вигляд:\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "Ця функція перетворює будь-яке дійсне значення $ z $ в інтервал від 0 до 1, що дозволяє інтерпретувати вихід як ймовірність для логістичної регресії.\n",
    "\n",
    "### Формула функції втрат для логістичної регресії (бінарна крос-ентропія):\n",
    "\n",
    "Функція втрат крос-ентропії оцінює, наскільки добре модель передбачає класи, порівнюючи передбачені ймовірності $ \\hat{y} $ із справжніми мітками $ y $. Формула наступна:\n",
    "\n",
    "$$\n",
    "L(y, \\hat{y}) = - \\left[ y \\cdot \\log(\\hat{y}) + (1 - y) \\cdot \\log(1 - \\hat{y}) \\right]\n",
    "$$\n",
    "\n",
    "Де:\n",
    "- $ y $ — це справжнє значення (мітка класу, 0 або 1).\n",
    "- $ \\hat{y} $ — це передбачене значення (ймовірність).\n",
    "\n"
   ],
   "metadata": {
    "id": "lbLHTNfSclli"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1.\n",
    "Тут вже наведений код для ініціювання набору даних в форматі numpy. Перетворіть `inputs`, `targets` на `torch` тензори. Виведіть результат на екран."
   ],
   "metadata": {
    "id": "GtOYB-RHfc_r"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.model_selection import learning_curve"
   ],
   "metadata": {
    "id": "3BNXSR-VdYKQ",
    "ExecuteTime": {
     "end_time": "2025-04-27T17:55:08.961625Z",
     "start_time": "2025-04-27T17:55:08.959545Z"
    }
   },
   "outputs": [],
   "execution_count": 148
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "QLKZ77x4v_-v",
    "ExecuteTime": {
     "end_time": "2025-04-27T17:55:08.988030Z",
     "start_time": "2025-04-27T17:55:08.985880Z"
    }
   },
   "source": [
    "# Вхідні дані (temp, rainfall, humidity)\n",
    "inputs = np.array([[73, 67, 43],\n",
    "                   [91, 88, 64],\n",
    "                   [87, 134, 58],\n",
    "                   [102, 43, 37],\n",
    "                   [69, 96, 70]], dtype='float32')\n",
    "\n",
    "# Таргети (apples > 80)\n",
    "targets = np.array([[0],\n",
    "                    [1],\n",
    "                    [1],\n",
    "                    [0],\n",
    "                    [1]], dtype='float32')"
   ],
   "outputs": [],
   "execution_count": 149
  },
  {
   "cell_type": "code",
   "source": [
    "inputs_tensor = torch.from_numpy(inputs)\n",
    "targets_tensor = torch.from_numpy(targets)"
   ],
   "metadata": {
    "id": "KjoeaDrk6fO7",
    "ExecuteTime": {
     "end_time": "2025-04-27T17:55:09.003572Z",
     "start_time": "2025-04-27T17:55:09.001597Z"
    }
   },
   "outputs": [],
   "execution_count": 150
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. Ініціюйте ваги `w`, `b` для моделі логістичної регресії потрібної форми зважаючи на розмірності даних випадковими значеннями з нормального розподілу. Лишаю тут код для фіксації `random_seed`."
   ],
   "metadata": {
    "id": "iKzbJKfOgGV8"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "torch.random.manual_seed(42)\n",
    "n_features = inputs_tensor.shape[1]\n",
    "\n",
    "w = torch.randn(n_features, 1, requires_grad=True)\n",
    "b = torch.randn(1, requires_grad=True)\n",
    "\n",
    "print(w.shape)\n",
    "print(b.shape)"
   ],
   "metadata": {
    "id": "aXhKw6Tdj1-d",
    "ExecuteTime": {
     "end_time": "2025-04-27T17:55:09.024722Z",
     "start_time": "2025-04-27T17:55:09.019836Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1])\n",
      "torch.Size([1])\n"
     ]
    }
   ],
   "execution_count": 151
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. Напишіть функцію `model`, яка буде обчислювати функцію гіпотези в логістичній регресії і дозволяти робити передбачення на основі введеного рядка даних і коефіцієнтів в змінних `w`, `b`.\n",
    "\n",
    "  **Важливий момент**, що функція `model` робить обчислення на `torch.tensors`, тож для математичних обчислень використовуємо фукнціонал `torch`, наприклад:\n",
    "  - обчсилення $e^x$: `torch.exp(x)`\n",
    "  - обчсилення $log(x)$: `torch.log(x)`\n",
    "  - обчислення середнього значення вектору `x`: `torch.mean(x)`\n",
    "\n",
    "  Використайте функцію `model` для обчислення передбачень з поточними значеннями `w`, `b`.Виведіть результат обчислень на екран.\n",
    "\n",
    "  Проаналізуйте передбачення. Чи не викликають вони у вас підозр? І якщо викликають, то чим це може бути зумовлено?"
   ],
   "metadata": {
    "id": "nYGxNGTaf5s6"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def model (x, w, b):\n",
    "    z = torch.matmul(x, w) + b\n",
    "    y_hat = 1 / (1 + torch.exp(-z))\n",
    "    return y_hat\n",
    "\n",
    "predictions = model(inputs_tensor, w, b)\n",
    "print(predictions)"
   ],
   "metadata": {
    "id": "pSz2j4Fh6jBv",
    "ExecuteTime": {
     "end_time": "2025-04-27T17:55:09.066272Z",
     "start_time": "2025-04-27T17:55:09.063343Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "execution_count": 152
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "It looks not OK. Probable reason could be wrong `w` and `b` initialization - probably they were too big."
  },
  {
   "cell_type": "markdown",
   "source": [
    "4. Напишіть функцію `binary_cross_entropy`, яка приймає на вхід передбачення моделі `predicted_probs` та справжні мітки в даних `true_labels` і обчислює значення втрат (loss)  за формулою бінарної крос-ентропії для кожного екземпляра та вертає середні втрати по всьому набору даних.\n",
    "  Використайте функцію `binary_cross_entropy` для обчислення втрат для поточних передбачень моделі."
   ],
   "metadata": {
    "id": "O2AGM0Mb2yHa"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def binary_cross_entropy(predicted_probs, true_labels):\n",
    "    # Clamp predictions to avoid log(0) error\n",
    "    eps = 1e-7\n",
    "    predicted_probs = torch.clamp(predicted_probs, eps, 1 - eps)\n",
    "\n",
    "    # Compute binary cross-entropy loss\n",
    "    loss = - (true_labels * torch.log(predicted_probs) + (1 - true_labels) * torch.log(1 - predicted_probs))\n",
    "\n",
    "    # Return average loss\n",
    "    return torch.mean(loss)\n",
    "\n",
    "# Calculate loss:\n",
    "loss = binary_cross_entropy(predictions, targets_tensor)\n",
    "print(f\"Binary Cross-Entropy Loss: {loss.item():.4f}\")"
   ],
   "metadata": {
    "id": "1bWlovvx6kZS",
    "ExecuteTime": {
     "end_time": "2025-04-27T17:55:09.119408Z",
     "start_time": "2025-04-27T17:55:09.116592Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Cross-Entropy Loss: 6.3770\n"
     ]
    }
   ],
   "execution_count": 153
  },
  {
   "cell_type": "markdown",
   "source": [
    "5. Зробіть зворотнє поширення помилки і виведіть градієнти за параметрами `w`, `b`. Проаналізуйте їх значення. Як гадаєте, чому вони саме такі?"
   ],
   "metadata": {
    "id": "ZFKpQxdHi1__"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "loss.backward()\n",
    "\n",
    "print(\"Gradient w.r.t weights (w):\")\n",
    "print(w.grad)\n",
    "\n",
    "print(\"\\nGradient w.r.t bias (b):\")\n",
    "print(b.grad)"
   ],
   "metadata": {
    "id": "YAbXUNSJ6mCl",
    "ExecuteTime": {
     "end_time": "2025-04-27T17:55:09.169382Z",
     "start_time": "2025-04-27T17:55:09.166677Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient w.r.t weights (w):\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.]])\n",
      "\n",
      "Gradient w.r.t bias (b):\n",
      "tensor([0.])\n"
     ]
    }
   ],
   "execution_count": 154
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Що сталось?**\n",
    "\n",
    "В цій задачі, коли ми ініціювали значення випадковими значеннями з нормального розподілу, насправді ці значення не були дуже гарними стартовими значеннями і привели до того, що градієнти стали дуже малими або навіть рівними нулю (це призводить до того, що градієнти \"зникають\"), і відповідно при оновленні ваг у нас не буде нічого змінюватись. Це називається `gradient vanishing`. Це відбувається через **насичення сигмоїдної функції активації.**\n",
    "\n",
    "У нашій задачі ми використовуємо сигмоїдну функцію активації, яка має такий вигляд:\n",
    "\n",
    "   $$\n",
    "   \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "   $$\n",
    "\n",
    "\n",
    "Коли значення $z$ дуже велике або дуже мале, сигмоїдна функція починає \"насичуватись\". Це означає, що для великих позитивних $z$ сигмоїда наближається до 1, а для великих негативних — до 0. В цих діапазонах градієнти починають стрімко зменшуватись і наближаються до нуля (бо градієнт - це похідна, похідна на проміжку функції, де вона паралельна осі ОХ, дорівнює 0), що робить оновлення ваг неможливим.\n",
    "\n",
    "![](https://editor.analyticsvidhya.com/uploads/27889vaegp.png)\n",
    "\n",
    "У логістичній регресії $ z = x \\cdot w + b $. Якщо ваги $w, b$ - великі, значення $z$ також буде великим, і сигмоїда перейде в насичену область, де градієнти дуже малі.\n",
    "\n",
    "Саме це сталося в нашій задачі, де великі випадкові значення ваг викликали насичення сигмоїдної функції. Це в свою чергу призводить до того, що під час зворотного поширення помилки (backpropagation) модель оновлює ваги дуже повільно або зовсім не оновлює. Це називається проблемою **зникнення градієнтів** (gradient vanishing problem).\n",
    "\n",
    "**Що ж робити?**\n",
    "Ініціювати ваги маленькими значеннями навколо нуля. Наприклад ми можемо просто в існуючій ініціалізації ваги розділити на 1000. Можна також використати інший спосіб ініціалізації вагів - інформація про це [тут](https://www.geeksforgeeks.org/initialize-weights-in-pytorch/).\n",
    "\n",
    "Як це робити - показую нижче. **Виконайте код та знову обчисліть передбачення, лосс і виведіть градієнти.**\n",
    "\n",
    "А я пишу пояснення, чому просто не зробити\n",
    "\n",
    "```\n",
    "w = torch.randn(1, 3, requires_grad=True)/1000\n",
    "b = torch.randn(1, requires_grad=True)/1000\n",
    "```\n",
    "\n",
    "Нам потрібно, аби тензори вагів були листовими (leaf tensors).\n",
    "\n",
    "1. **Що таке листовий тензор**\n",
    "Листовий тензор — це тензор, який був створений користувачем безпосередньо і з якого починається обчислювальний граф. Якщо такий тензор має `requires_grad=True`, PyTorch буде відслідковувати всі операції, виконані над ним, щоб правильно обчислювати градієнти під час навчання.\n",
    "\n",
    "2. **Чому ми використовуємо `w.data` замість звичайних операцій**\n",
    "Якщо ми просто виконали б операції, такі як `(w - 0.5) / 100`, ми б отримали **новий тензор**, який вже не був би листовим тензором, оскільки ці операції створюють **новий** тензор, а не модифікують існуючий.\n",
    "\n",
    "  Проте, щоб залишити наші тензори ваги `w` та зміщення `b` листовими і продовжити можливість відстеження градієнтів під час тренування, ми використовуємо атрибут `.data`. Цей атрибут дозволяє **виконувати операції in-place (прямо на існуючому тензорі)** без зміни самого об'єкта тензора. Отже, тензор залишається листовим, і PyTorch може коректно обчислювати його градієнти.\n",
    "\n",
    "3. **Чому важливо залишити тензор листовим**\n",
    "Якщо тензор більше не є листовим (наприклад, через проведення операцій, що створюють нові тензори), ви не зможете отримати градієнти за допомогою `w.grad` чи `b.grad` після виклику `loss.backward()`. Це може призвести до втрати можливості оновлення параметрів під час тренування моделі. В нашому випадку ми хочемо, щоб тензори `w` та `b` накопичували градієнти, тому вони повинні залишатись листовими.\n",
    "\n",
    "**Висновок:**\n",
    "Ми використовуємо `.data`, щоб виконати операції зміни значень на ваги і зміщення **in-place**, залишаючи їх листовими тензорами, які можуть накопичувати градієнти під час навчання. Це дозволяє коректно працювати механізму зворотного поширення помилки (backpropagation) і оновлювати ваги моделі."
   ],
   "metadata": {
    "id": "nDN1t1RujQsK"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "5. Виконайте код та знову обчисліть передбачення, лосс і знайдіть градієнти та виведіть всі ці тензори на екран."
   ],
   "metadata": {
    "id": "rOPSQyttpVjO"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "torch.random.manual_seed(1)\n",
    "w = torch.randn(n_features, 1, requires_grad=True)\n",
    "b = torch.randn(1, requires_grad=True)\n",
    "\n",
    "# in-place операції\n",
    "w.data = w.data / 1000\n",
    "b.data = b.data / 1000"
   ],
   "metadata": {
    "id": "-EBOJ3tsnRaD",
    "ExecuteTime": {
     "end_time": "2025-04-27T17:55:09.216087Z",
     "start_time": "2025-04-27T17:55:09.213416Z"
    }
   },
   "outputs": [],
   "execution_count": 155
  },
  {
   "cell_type": "code",
   "source": [
    "predictions = model(inputs_tensor, w, b)\n",
    "print(predictions)\n",
    "\n",
    "# Calculate loss:\n",
    "loss = binary_cross_entropy(predictions, targets_tensor)\n",
    "print(f\"Binary Cross-Entropy Loss: {loss.item():.4f}\")"
   ],
   "metadata": {
    "id": "-JwXiSpX6orh",
    "ExecuteTime": {
     "end_time": "2025-04-27T17:55:09.247402Z",
     "start_time": "2025-04-27T17:55:09.244271Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5174],\n",
      "        [0.5220],\n",
      "        [0.5244],\n",
      "        [0.5204],\n",
      "        [0.5190]], grad_fn=<MulBackward0>)\n",
      "Binary Cross-Entropy Loss: 0.6829\n"
     ]
    }
   ],
   "execution_count": 156
  },
  {
   "cell_type": "markdown",
   "source": [
    "6. Напишіть алгоритм градієнтного спуску, який буде навчати модель з використанням написаних раніше функцій і виконуючи оновлення ваг. Алгоритм має включати наступні кроки:\n",
    "\n",
    "  - Генерація прогнозів\n",
    "  - Обчислення втрат\n",
    "  - Обчислення градієнтів (gradients) loss-фукнції відносно ваг і зсувів\n",
    "  - Налаштування ваг шляхом віднімання невеликої величини, пропорційної градієнту (`learning_rate` домножений на градієнт)\n",
    "  - Скидання градієнтів на нуль\n",
    "\n",
    "Виконайте градієнтний спуск протягом 1000 епох, обчисліть фінальні передбачення і проаналізуйте, чи вони точні?"
   ],
   "metadata": {
    "id": "RCdi44IT334o"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "w = torch.randn(n_features, 1) * 0.01\n",
    "w.requires_grad_()\n",
    "b = torch.randn(1) * 0.01\n",
    "b.requires_grad_()\n",
    "\n",
    "inputs_tensor = (inputs_tensor - inputs_tensor.mean(dim=0)) / inputs_tensor.std(dim=0)\n",
    "\n",
    "learning_rate = 1.0\n",
    "num_epochs = 1000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    predictions = model(inputs_tensor, w, b)\n",
    "    loss = binary_cross_entropy(predictions, targets_tensor)\n",
    "    loss.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad\n",
    "        b -= learning_rate * b.grad\n",
    "\n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "\n",
    "    if(epoch+1) % 100 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "final_predictions = model(inputs_tensor, w, b)\n",
    "print(final_predictions)"
   ],
   "metadata": {
    "id": "mObHPyE06qsO",
    "ExecuteTime": {
     "end_time": "2025-04-27T17:55:09.398397Z",
     "start_time": "2025-04-27T17:55:09.291407Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/1000, Loss: 0.0110\n",
      "Epoch 200/1000, Loss: 0.0055\n",
      "Epoch 300/1000, Loss: 0.0037\n",
      "Epoch 400/1000, Loss: 0.0027\n",
      "Epoch 500/1000, Loss: 0.0022\n",
      "Epoch 600/1000, Loss: 0.0018\n",
      "Epoch 700/1000, Loss: 0.0016\n",
      "Epoch 800/1000, Loss: 0.0014\n",
      "Epoch 900/1000, Loss: 0.0012\n",
      "Epoch 1000/1000, Loss: 0.0011\n",
      "tensor([[1.9320e-03],\n",
      "        [9.9782e-01],\n",
      "        [9.9970e-01],\n",
      "        [2.6456e-04],\n",
      "        [9.9922e-01]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "execution_count": 157
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Секція 2. Створення лог регресії з використанням функціоналу `torch.nn`.**\n",
    "\n",
    "Давайте повторно реалізуємо ту ж модель, використовуючи деякі вбудовані функції та класи з PyTorch.\n",
    "\n",
    "Даних у нас буде побільше - тож, визначаємо нові масиви."
   ],
   "metadata": {
    "id": "fuRhlyF9qAia"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Вхідні дані (temp, rainfall, humidity)\n",
    "inputs = np.array([[73, 67, 43],\n",
    "                   [91, 88, 64],\n",
    "                   [87, 134, 58],\n",
    "                   [102, 43, 37],\n",
    "                   [69, 96, 70],\n",
    "                   [73, 67, 43],\n",
    "                   [91, 88, 64],\n",
    "                   [87, 134, 58],\n",
    "                   [102, 43, 37],\n",
    "                   [69, 96, 70],\n",
    "                   [73, 67, 43],\n",
    "                   [91, 88, 64],\n",
    "                   [87, 134, 58],\n",
    "                   [102, 43, 37],\n",
    "                   [69, 96, 70]], dtype='float32')\n",
    "\n",
    "# Таргети (apples > 80)\n",
    "targets = np.array([[0],\n",
    "                    [1],\n",
    "                    [1],\n",
    "                    [0],\n",
    "                    [1],\n",
    "                    [0],\n",
    "                    [1],\n",
    "                    [1],\n",
    "                    [0],\n",
    "                    [1],\n",
    "                    [0],\n",
    "                    [1],\n",
    "                    [1],\n",
    "                    [0],\n",
    "                    [1]], dtype='float32')"
   ],
   "metadata": {
    "id": "IX8Bhm74rV4M",
    "ExecuteTime": {
     "end_time": "2025-04-27T17:55:09.416561Z",
     "start_time": "2025-04-27T17:55:09.413952Z"
    }
   },
   "outputs": [],
   "execution_count": 158
  },
  {
   "cell_type": "markdown",
   "source": [
    "7. Завантажте вхідні дані та мітки в PyTorch тензори та з них створіть датасет, який поєднує вхідні дані з мітками, використовуючи клас `TensorDataset`. Виведіть перші 3 елементи в датасеті.\n",
    "\n"
   ],
   "metadata": {
    "id": "7X2dV30KtAPu"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "inputs_tensor = torch.from_numpy(inputs)\n",
    "targets_tensor = torch.from_numpy(targets)\n",
    "\n",
    "inputs_tensor = (inputs_tensor - inputs_tensor.mean(dim=0)) / inputs_tensor.std(dim=0)\n",
    "\n",
    "dataset = TensorDataset(inputs_tensor, targets_tensor)"
   ],
   "metadata": {
    "id": "chrvMfBs6vjo",
    "ExecuteTime": {
     "end_time": "2025-04-27T17:55:09.429630Z",
     "start_time": "2025-04-27T17:55:09.427276Z"
    }
   },
   "outputs": [],
   "execution_count": 159
  },
  {
   "cell_type": "markdown",
   "source": [
    "8. Визначте data loader з класом **DataLoader** для підготовленого датасету `train_ds`, встановіть розмір батчу на 5 та увімкніть перемішування даних для ефективного навчання моделі. Виведіть перший елемент в дата лоадері."
   ],
   "metadata": {
    "id": "4nMFaa8suOd3"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 5\n",
    "train_dl = DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "for batch_inputs, batch_targets in train_dl:\n",
    "    print(\"Batch inputs:\", batch_inputs)\n",
    "    print(\"Batch targets:\", batch_targets)\n",
    "    break"
   ],
   "metadata": {
    "id": "ZCsRo5Mx6wEI",
    "ExecuteTime": {
     "end_time": "2025-04-27T17:55:09.447753Z",
     "start_time": "2025-04-27T17:55:09.444388Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch inputs: tensor([[ 1.4099, -1.3543, -1.3448],\n",
      "        [ 0.5287,  0.0763,  0.7420],\n",
      "        [-0.9132, -0.5913, -0.8811],\n",
      "        [ 0.2083,  1.5387,  0.2782],\n",
      "        [ 0.2083,  1.5387,  0.2782]])\n",
      "Batch targets: tensor([[0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.]])\n"
     ]
    }
   ],
   "execution_count": 160
  },
  {
   "cell_type": "markdown",
   "source": [
    "9. Створіть клас `LogReg` для логістичної регресії, наслідуючи модуль `torch.nn.Module` за прикладом в лекції (в частині про FeedForward мережі).\n",
    "\n",
    "  У нас модель складається з лінійної комбінації вхідних значень і застосування фукнції сигмоїда. Тож, нейромережа буде складатись з лінійного шару `nn.Linear` і використання активації `nn.Sigmid`. У створеному класі мають бути реалізовані методи `__init__` з ініціалізацією шарів і метод `forward` для виконання прямого проходу моделі через лінійний шар і функцію активації.\n",
    "\n",
    "  Створіть екземпляр класу `LogReg` в змінній `model`."
   ],
   "metadata": {
    "id": "ymcQOo_hum6I"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LogReg(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(LogReg, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "input_dim = inputs_tensor.shape[1]\n",
    "model = LogReg(input_dim)\n",
    "\n",
    "print(model)"
   ],
   "metadata": {
    "id": "EyAwhTBW6xxz",
    "ExecuteTime": {
     "end_time": "2025-04-27T17:56:43.393164Z",
     "start_time": "2025-04-27T17:56:43.388865Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogReg(\n",
      "  (linear): Linear(in_features=3, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 165
  },
  {
   "cell_type": "markdown",
   "source": [
    "10. Задайте оптимізатор `Stockastic Gradient Descent` в змінній `opt` для навчання моделі логістичної регресії. А також визначіть в змінній `loss` функцію втрат `binary_cross_entropy` з модуля `torch.nn.functional` для обчислення втрат моделі. Обчисліть втрати для поточних передбачень і міток, а потім виведіть їх. Зробіть висновок, чи моделі вдалось навчитись?"
   ],
   "metadata": {
    "id": "RflV7xeVyoJy"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "opt = optim.SGD(model.parameters(), lr=0.01)\n",
    "loss_fn = F.binary_cross_entropy\n",
    "\n",
    "for xb, yb, in train_dl:\n",
    "    preds = model(xb)\n",
    "    break\n",
    "\n",
    "initial_loss = loss_fn(preds, yb)\n",
    "\n",
    "print(f\"Initial loss: {initial_loss.item():.4f}\")"
   ],
   "metadata": {
    "id": "3QCATPU_6yfa",
    "ExecuteTime": {
     "end_time": "2025-04-27T17:56:45.160534Z",
     "start_time": "2025-04-27T17:56:45.155634Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss: 0.7307\n"
     ]
    }
   ],
   "execution_count": 166
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The initial loss was 0.7307.\n",
    "\n",
    "This value is quite high, which is expected because the model weights are random, and no training has been performed yet. Therefore, the model has not learned yet.\n",
    "\n",
    "We need to perform several epochs of training with gradient descent to reduce the loss and improve predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "11. Візьміть з лекції функцію для тренування моделі з відстеженням значень втрат і навчіть щойно визначену модель на 1000 епохах. Виведіть після цього графік зміни loss, фінальні передбачення і значення таргетів."
   ],
   "metadata": {
    "id": "ch-WrYnKzMzq"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def fit_return_loss(num_epochs, model, loss_fn, opt, train_dl):\n",
    "    losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        # Ініціалізуємо акумулятор для втрат\n",
    "        total_loss = 0\n",
    "\n",
    "        for xb, yb in train_dl:\n",
    "            # Генеруємо передбачення\n",
    "            pred = model(xb)\n",
    "\n",
    "            # Обчислюємо втрати\n",
    "            loss = loss_fn(pred, yb)\n",
    "\n",
    "            # Виконуємо градієнтний спуск\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "\n",
    "            # Накопичуємо втрати\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # Обчислюємо середні втрати для епохи\n",
    "        avg_loss = total_loss / len(train_dl)\n",
    "        losses.append(avg_loss)\n",
    "\n",
    "        # Виводимо підсумок епохи\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "          print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n",
    "    return losses\n",
    "\n",
    "# 1. Train the model\n",
    "num_epochs = 1000\n",
    "losses = fit_return_loss(num_epochs, model, loss_fn, opt, train_dl)\n",
    "\n",
    "# 2. Plot the loss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss over Epochs')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# 3. Final predictions\n",
    "for xb, yb in train_dl:\n",
    "    final_preds = model(xb)\n",
    "    break\n",
    "\n",
    "print(\"Final predictions:\", final_preds.detach().numpy().round())\n",
    "print(\"True targets:\", yb.numpy())"
   ],
   "metadata": {
    "id": "cEHQH9qE626k",
    "ExecuteTime": {
     "end_time": "2025-04-27T18:01:54.011108Z",
     "start_time": "2025-04-27T18:01:53.191343Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/1000], Loss: 0.6210\n",
      "Epoch [20/1000], Loss: 0.5285\n",
      "Epoch [30/1000], Loss: 0.4567\n",
      "Epoch [40/1000], Loss: 0.4005\n",
      "Epoch [50/1000], Loss: 0.3557\n",
      "Epoch [60/1000], Loss: 0.3196\n",
      "Epoch [70/1000], Loss: 0.2897\n",
      "Epoch [80/1000], Loss: 0.2649\n",
      "Epoch [90/1000], Loss: 0.2438\n",
      "Epoch [100/1000], Loss: 0.2259\n",
      "Epoch [110/1000], Loss: 0.2105\n",
      "Epoch [120/1000], Loss: 0.1970\n",
      "Epoch [130/1000], Loss: 0.1851\n",
      "Epoch [140/1000], Loss: 0.1747\n",
      "Epoch [150/1000], Loss: 0.1653\n",
      "Epoch [160/1000], Loss: 0.1569\n",
      "Epoch [170/1000], Loss: 0.1494\n",
      "Epoch [180/1000], Loss: 0.1425\n",
      "Epoch [190/1000], Loss: 0.1363\n",
      "Epoch [200/1000], Loss: 0.1306\n",
      "Epoch [210/1000], Loss: 0.1253\n",
      "Epoch [220/1000], Loss: 0.1205\n",
      "Epoch [230/1000], Loss: 0.1160\n",
      "Epoch [240/1000], Loss: 0.1119\n",
      "Epoch [250/1000], Loss: 0.1080\n",
      "Epoch [260/1000], Loss: 0.1044\n",
      "Epoch [270/1000], Loss: 0.1011\n",
      "Epoch [280/1000], Loss: 0.0979\n",
      "Epoch [290/1000], Loss: 0.0949\n",
      "Epoch [300/1000], Loss: 0.0922\n",
      "Epoch [310/1000], Loss: 0.0895\n",
      "Epoch [320/1000], Loss: 0.0871\n",
      "Epoch [330/1000], Loss: 0.0847\n",
      "Epoch [340/1000], Loss: 0.0825\n",
      "Epoch [350/1000], Loss: 0.0804\n",
      "Epoch [360/1000], Loss: 0.0784\n",
      "Epoch [370/1000], Loss: 0.0765\n",
      "Epoch [380/1000], Loss: 0.0747\n",
      "Epoch [390/1000], Loss: 0.0730\n",
      "Epoch [400/1000], Loss: 0.0713\n",
      "Epoch [410/1000], Loss: 0.0697\n",
      "Epoch [420/1000], Loss: 0.0682\n",
      "Epoch [430/1000], Loss: 0.0668\n",
      "Epoch [440/1000], Loss: 0.0654\n",
      "Epoch [450/1000], Loss: 0.0641\n",
      "Epoch [460/1000], Loss: 0.0628\n",
      "Epoch [470/1000], Loss: 0.0616\n",
      "Epoch [480/1000], Loss: 0.0604\n",
      "Epoch [490/1000], Loss: 0.0593\n",
      "Epoch [500/1000], Loss: 0.0582\n",
      "Epoch [510/1000], Loss: 0.0571\n",
      "Epoch [520/1000], Loss: 0.0561\n",
      "Epoch [530/1000], Loss: 0.0552\n",
      "Epoch [540/1000], Loss: 0.0542\n",
      "Epoch [550/1000], Loss: 0.0533\n",
      "Epoch [560/1000], Loss: 0.0524\n",
      "Epoch [570/1000], Loss: 0.0516\n",
      "Epoch [580/1000], Loss: 0.0507\n",
      "Epoch [590/1000], Loss: 0.0499\n",
      "Epoch [600/1000], Loss: 0.0492\n",
      "Epoch [610/1000], Loss: 0.0484\n",
      "Epoch [620/1000], Loss: 0.0477\n",
      "Epoch [630/1000], Loss: 0.0470\n",
      "Epoch [640/1000], Loss: 0.0463\n",
      "Epoch [650/1000], Loss: 0.0456\n",
      "Epoch [660/1000], Loss: 0.0450\n",
      "Epoch [670/1000], Loss: 0.0443\n",
      "Epoch [680/1000], Loss: 0.0437\n",
      "Epoch [690/1000], Loss: 0.0431\n",
      "Epoch [700/1000], Loss: 0.0426\n",
      "Epoch [710/1000], Loss: 0.0420\n",
      "Epoch [720/1000], Loss: 0.0414\n",
      "Epoch [730/1000], Loss: 0.0409\n",
      "Epoch [740/1000], Loss: 0.0404\n",
      "Epoch [750/1000], Loss: 0.0399\n",
      "Epoch [760/1000], Loss: 0.0394\n",
      "Epoch [770/1000], Loss: 0.0389\n",
      "Epoch [780/1000], Loss: 0.0384\n",
      "Epoch [790/1000], Loss: 0.0380\n",
      "Epoch [800/1000], Loss: 0.0375\n",
      "Epoch [810/1000], Loss: 0.0371\n",
      "Epoch [820/1000], Loss: 0.0366\n",
      "Epoch [830/1000], Loss: 0.0362\n",
      "Epoch [840/1000], Loss: 0.0358\n",
      "Epoch [850/1000], Loss: 0.0354\n",
      "Epoch [860/1000], Loss: 0.0350\n",
      "Epoch [870/1000], Loss: 0.0346\n",
      "Epoch [880/1000], Loss: 0.0343\n",
      "Epoch [890/1000], Loss: 0.0339\n",
      "Epoch [900/1000], Loss: 0.0335\n",
      "Epoch [910/1000], Loss: 0.0332\n",
      "Epoch [920/1000], Loss: 0.0328\n",
      "Epoch [930/1000], Loss: 0.0325\n",
      "Epoch [940/1000], Loss: 0.0322\n",
      "Epoch [950/1000], Loss: 0.0318\n",
      "Epoch [960/1000], Loss: 0.0315\n",
      "Epoch [970/1000], Loss: 0.0312\n",
      "Epoch [980/1000], Loss: 0.0309\n",
      "Epoch [990/1000], Loss: 0.0306\n",
      "Epoch [1000/1000], Loss: 0.0303\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final predictions: [[1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]]\n",
      "True targets: [[1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "execution_count": 167
  }
 ]
}
