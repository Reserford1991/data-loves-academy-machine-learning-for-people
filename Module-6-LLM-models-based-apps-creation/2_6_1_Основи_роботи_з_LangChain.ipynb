{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CONjBYGh0bY"
      },
      "source": [
        "# Основи роботи з LangChain\n",
        "\n",
        "LangChain — це фреймворк, створений для полегшення побудови застосунків на основі великих мовних моделей (LLMs). Основною ідеєю LangChain є інтеграція LLM у більш складні та персоналізовані робочі процеси, що дозволяє використовувати мовні моделі не просто для генерації тексту, але і для виконання складних бізнес-завдань або автоматизації процесів.\n",
        "\n",
        "LangChain дозволяє поєднувати LLM з зовнішніми інструментами, базами даних або API. Завдяки цьому можна створювати складніші сценарії використання, наприклад:\n",
        "\n",
        "1. **Інтеграція з базами даних**: LangChain може працювати з великими обсягами даних або підключатися до зовнішніх джерел інформації, надаючи змогу LLM доступати до актуальних або специфічних даних.\n",
        "  \n",
        "2. **Запам'ятовування контексту**: Зазвичай LLM не має \"пам'яті\", але LangChain дозволяє зберігати стан і контекст між запитами, що корисно для чат-ботів або додатків, які вимагають пам’яті про попередні запити або сесії, однак LangChain може допомогти реалізувати механізми збереження контексту розмови або дій. Це дозволяє моделі пам’ятати попередні відповіді або завдання, роблячи роботу з LLM більш цілісною і логічною в довгих сесіях.\n",
        "\n",
        "3. **Розгалужені ланцюги дій**: За допомогою LangChain можна побудувати ланцюжки дій, де LLM виконує серію кроків або запитів, заснованих на попередніх результатах. Це корисно для складних задач, які вимагають кількох етапів обробки даних або прийняття рішень.\n",
        "\n",
        "4. **Інтеграція з іншими інструментами**: LangChain дозволяє легко підключати сторонні інструменти, такі як інтерфейси користувача, системи аналітики або автоматизації бізнес-процесів, що розширює можливості моделі.\n",
        "\n",
        "5. **Індивідуалізація застосунків**: Розробники можуть створювати спеціалізовані програми для виконання певних завдань на основі даних або бізнес-потреб, таких як чат-боти, автоматизовані системи підтримки клієнтів або моделі рекомендацій.\n",
        "\n",
        "Таким чином, **LangChain** — це інструмент, який робить LLM більш гнучкими та функціональними в реальних умовах, дозволяючи будувати застосунки на основі штучного інтелекту з урахуванням складних сценаріїв і бізнес-потреб."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbYVgIZchwfq"
      },
      "source": [
        "Встановлення всіх необхідних пакетів..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRYSu48huSUW"
      },
      "outputs": [],
      "source": [
        "!pip -q install langchain langchain_openai huggingface_hub openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6yiwXNnvzxO"
      },
      "source": [
        "#### Налаштування моделі OpenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2PbNnQhqzAL"
      },
      "source": [
        "Нам потрібно завантажити деякі токени. Я помістила їх у файл наступної структури:\n",
        "```\n",
        "{\n",
        "\"OPENAI_API_KEY\": \"...\",\n",
        "\"HUGGINGFACEHUB_API_TOKEN\": \"...\",\n",
        "\"SERPAPI_API_KEY\":\"...\"\n",
        "}\n",
        "\n",
        "```\n",
        "Потім я просто додала цей файл до контексту цього ноутбука."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J1rye0_OetAU"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "with open('creds.json') as file:\n",
        "  creds = json.load(file)\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = creds[\"OPENAI_API_KEY\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaabjHKmh7_m"
      },
      "source": [
        "Для більшості LLM (великі мовні моделі) ми можемо налаштувати параметр температури, який контролює креативність тексту, що генерується API OpenAI. Вища температура призведе до більш креативного тексту, тоді як нижча температура створить більш передбачуваний текст.\n",
        "\n",
        "Давайте зробимо наші виходи досить передбачуваними, але з невеликою часткою креативності.\n",
        "\n",
        "Значення за замовчуванням для температури зазвичай становить 0.7."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mu6rG38mhFYo"
      },
      "outputs": [],
      "source": [
        "overal_temperature = 0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wYGExb_ioxi"
      },
      "source": [
        "Ви можете знайти опис параметрів моделей OpenAI [тут](https://python.langchain.com/docs/concepts/#chat-models).\n",
        "\n",
        "А загальну інформацію щодо моделей OpenAI з інтеграцією LangChain - [тут](https://python.langchain.com/docs/integrations/llms/openai)\n",
        "\n",
        "Сорс код класа `ChatOpenAI`: [тут](https://github.com/langchain-ai/langchain/blob/0640cbf2f126f773b7ae78b0f94c1ba0caabb2c1/libs/community/langchain_community/chat_models/openai.py#L180)\n",
        "\n",
        "## Різні OpenAI моделі\n",
        "\n",
        "У фреймворку **LangChain** існують дві основні класи для роботи з моделями OpenAI: **ChatOpenAI** та **OpenAI**. Ось їх основні відмінності:\n",
        "\n",
        "### 1. **OpenAI** (клас для роботи з мовними моделями):\n",
        "- **OpenAI** використовується для взаємодії з текстовими мовними моделями, такими як GPT-3 або GPT-4, які приймають текст як вхід і генерують текст у відповідь.\n",
        "- Цей клас добре підходить для виконання стандартних запитів, де ви передаєте текст і отримуєте текстовий результат.\n",
        "- **OpenAI** не підтримує багаторівневу структуру розмови чи контекстний діалог; це просто запит-відповідь без збереження історії діалогу.\n",
        "\n",
        "\n",
        "### 2. **ChatOpenAI** (клас для роботи з чат-моделями):\n",
        "- **ChatOpenAI** використовується для роботи з моделями, оптимізованими для чат-сценаріїв, такими як GPT-4, які можуть вести розмови та обробляти кілька повідомлень з діалогу.\n",
        "- Цей клас дозволяє працювати з історією чату, надаючи можливість передавати повідомлення як частину розмови, включаючи повідомлення від користувача (Human) і відповіді моделі (AI).\n",
        "- **ChatOpenAI** також краще підходить для додатків, де важливо зберігати контекст діалогу та попередні повідомлення.\n",
        "\n",
        "Таким чином, **ChatOpenAI** краще використовувати, коли потрібно будувати чат-ботів або вести розмову, тоді як **OpenAI** підходить для одноразових генерацій тексту на основі вхідного запиту."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-lzO5PfUpwfv"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import OpenAI\n",
        "\n",
        "llm = OpenAI(temperature=overal_temperature)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm"
      ],
      "metadata": {
        "id": "DU7f8C6ovNcU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMaNWpGMjhc7"
      },
      "source": [
        "### Отримання прогнозів"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fI_Ve5bjjA2D"
      },
      "outputs": [],
      "source": [
        "llm.invoke('who am I talking to?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqF-YPVEi7jZ"
      },
      "outputs": [],
      "source": [
        "request = \"What are 5 vacation destinations for someone who likes to eat pasta?\"\n",
        "print(llm.invoke(request))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "А ось таким чином можна реалізувати стрімінг - це якщо ви захочете написати свого чатбота:"
      ],
      "metadata": {
        "id": "dsvW_bewZGfC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for chunk in llm.stream(\n",
        "    \"What are some theories about the relationship between unemployment and inflation?\"\n",
        "):\n",
        "    print(chunk, end=\"\", flush=True)"
      ],
      "metadata": {
        "id": "Ao3WkMGrZD4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Більше про різні інтерфеси як ранити LLM з langchain - [тут](https://python.langchain.com/docs/integrations/llms/openai/)"
      ],
      "metadata": {
        "id": "z2jH0-kqZNGE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulXC2DyLjtw7"
      },
      "source": [
        "### Промпти: Керування запитами для LLMs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MP2yGFHej1Qr"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pPq0lUpoj33z"
      },
      "outputs": [],
      "source": [
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"food\"],\n",
        "    template=\"What are 5 vacation destinations for someone who likes to eat {food}?\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IPTvCstej7WP"
      },
      "outputs": [],
      "source": [
        "print(prompt.format(food=\"donuts\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qA1FWh13j-uH"
      },
      "outputs": [],
      "source": [
        "print(llm.invoke(prompt.format(food=\"donuts\")))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbelXktTkzRq"
      },
      "source": [
        "### Ланцюги: Поєднання LLM (великі мовні моделі) та запитів у багатоступеневих робочих процесах"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Організувтати ланцюг дуже просто:"
      ],
      "metadata": {
        "id": "rjQFU8f0YL4I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0jyUSrYpknCV"
      },
      "outputs": [],
      "source": [
        "chain = prompt | llm\n",
        "print(chain.invoke(\"fresh fish\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQxNVxu9lU0S"
      },
      "source": [
        "### Агенти: Можливість динамічно викликати ланцюги на основі введення користувача\n",
        "\n",
        "**Агент**\n",
        "\n",
        "Як зазначено в документації, Агент в LangChain охоплює наступні абстракції:\n",
        "\n",
        "1. **AgentAction**: Представляє наступну дію, яку необхідно виконати, що складається з інструменту та вхідних даних для інструменту.\n",
        "   \n",
        "2. **AgentFinish**: Остаточний результат від агента, що містить фінальний результат агента в `return_values`.\n",
        "   \n",
        "3. **Intermediate Steps**: Означає попередні дії агента та їх відповідні результати, організовані як список кортежів `[AgentAction, Any]`.\n",
        "\n",
        "Ця структура вказує на те, що для запиту може бути виконано кілька дій агента за потреби, при цьому проміжні дії зберігаються в проміжних кроках.\n",
        "\n",
        "**AgentExecutor**\n",
        "\n",
        "AgentExecutor відповідає за використання Агента до тих пір, поки не буде отримано остаточний результат. Таким чином, він використовує Агента для отримання наступної дії, виконує повернуту дію поетапно та продовжує цей процес, доки не буде згенеровано остаточну відповідь для даного вводу.\n",
        "\n",
        "Раптом вам цікаов, як влаштовані технічно агенти під капотом:\n",
        "![](https://miro.medium.com/v2/resize:fit:4800/format:webp/1*CD2Svi5BUZaG-d-KWeU3ug.png)\n",
        "\n",
        "Детально про те, як під капотом працюють агенти можна прочитати [тут](https://nakamasato.medium.com/langchain-how-an-agent-works-7dce1569933d)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3_j3IHYlXT7"
      },
      "outputs": [],
      "source": [
        "!pip install -q google-search-results langchain-community langchain_experimental"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "852Ucalcrgh7"
      },
      "source": [
        "Щоб зробити API пошуку доступним (100 пошуків на місяць), просто зареєструйтесь [тут](https://serpapi.com/users/welcome).\n",
        "Ми будемо використовувати [ReAct](https://react-lm.github.io/) агента. ReAct від Reasoning-Action. Агенти ReAct від LangChain допомагають організувати весь процес обробки запитів. Використовуючи ці агенти, ми можемо розбити складні запити на керовані кроки і виконувати їх систематично. Агент задається спеціальним промптом, який ми далі з вами розглянемо."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CeVfjVALlUjl"
      },
      "outputs": [],
      "source": [
        "from langchain import hub\n",
        "from langchain.agents import load_tools\n",
        "from langchain.agents import Tool, AgentExecutor, AgentType, create_react_agent, initialize_agent\n",
        "from langchain.chains import LLMMathChain\n",
        "from langchain_experimental.utilities import PythonREPL"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=overal_temperature)\n",
        "problem_chain = LLMMathChain.from_llm(llm=llm)\n",
        "math_tool = Tool.from_function(name=\"Calculator\",\n",
        "                func=problem_chain.run,\n",
        "                description=\"Useful for when you need to answer questions about math. This tool is only for math questions and nothing else. Only input math expressions.\")"
      ],
      "metadata": {
        "id": "hOVsID0Z5dNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "problem_chain.invoke('(112*132)-19/3')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwWXeZr57519",
        "outputId": "894519cf-d87a-44e3-b688-000554471467"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'question': '(112*132)-19/3', 'answer': 'Answer: 14777.666666666666'}"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm.invoke('(112*132)-19/3')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUnH0xyCGeKy",
        "outputId": "51233647-af8b-471d-8df6-b3e617895e02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='To compute \\\\( (112 \\\\times 132) - \\\\frac{19}{3} \\\\), we will follow the order of operations.\\n\\n1. First, calculate \\\\( 112 \\\\times 132 \\\\):\\n   \\\\[\\n   112 \\\\times 132 = 14784\\n   \\\\]\\n\\n2. Next, calculate \\\\( \\\\frac{19}{3} \\\\):\\n   \\\\[\\n   \\\\frac{19}{3} \\\\approx 6.3333\\n   \\\\]\\n\\n3. Now, subtract \\\\( \\\\frac{19}{3} \\\\) from \\\\( 14784 \\\\):\\n   \\\\[\\n   14784 - \\\\frac{19}{3} = 14784 - 6.3333 \\\\approx 14777.6667\\n   \\\\]\\n\\nThus, the final result is approximately \\\\( 14777.67 \\\\).', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 171, 'prompt_tokens': 15, 'total_tokens': 186, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_482c22a7bc', 'finish_reason': 'stop', 'logprobs': None}, id='run-cc4ed400-3354-4b81-af66-9f534343cebf-0', usage_metadata={'input_tokens': 15, 'output_tokens': 171, 'total_tokens': 186, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(112*132)-19/3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SFcOoQZGX_y",
        "outputId": "3dfc0995-eda8-4fa8-877d-3e89f5cd53c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14777.666666666666"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "REPL (Read-Eval-Print Loop) — це інтерактивне середовище програмування, яке дозволяє користувачу вводити код, отримувати його результат і продовжувати виконання в режимі реального часу. По суті, це цикл, який \"читає\" введення, \"оцінює\" його, \"друкує\" результат і чекає на новий введений код. REPL часто використовується в мовах програмування для швидкого тестування й налагодження коду, таких як Python, JavaScript тощо."
      ],
      "metadata": {
        "id": "nifd_Rxrpzih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "python_repl = PythonREPL()\n",
        "python_tool = Tool(\n",
        "    name=\"python_repl\",\n",
        "    description=\"A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you necessarily should print it out with `print(...)`. Otherwise you won't see the result! It's very important.\",\n",
        "    func=python_repl.run,\n",
        ")\n",
        "python_tool.name = \"python_interpreter\""
      ],
      "metadata": {
        "id": "L1yo3LLV7qDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "python_repl.run(\"import pandas as pd; s = pd.Series([1,2,3])\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "8Lz9wnxp72Ro",
        "outputId": "ac6f6955-f412-4b4d-9b0f-f23094fe1a09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "''"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OepTDXlQmvvu"
      },
      "outputs": [],
      "source": [
        "os.environ[\"SERPAPI_API_KEY\"] = creds[\"SERPAPI_API_KEY\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhrz11BSlhkl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7084d79f-96de-499c-80e0-b0825bbfb0fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langsmith/client.py:354: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "tools = load_tools([\"serpapi\"], llm=llm)\n",
        "prompt = hub.pull(\"hwchase17/react\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tools"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wc04KCgjHlkO",
        "outputId": "2cb447d8-67b2-494f-dce5-d19699d20c64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Tool(name='Search', description='A search engine. Useful for when you need to answer questions about current events. Input should be a search query.', func=<bound method SerpAPIWrapper.run of SerpAPIWrapper(search_engine=<class 'serpapi.google_search.GoogleSearch'>, params={'engine': 'google', 'google_domain': 'google.com', 'gl': 'us', 'hl': 'en'}, serpapi_api_key='75edf52f8da893b36fb5ca9639c81e6edf8b2bc3df0c58d180428ba3bdc8c686', aiosession=None)>, coroutine=<bound method SerpAPIWrapper.arun of SerpAPIWrapper(search_engine=<class 'serpapi.google_search.GoogleSearch'>, params={'engine': 'google', 'google_domain': 'google.com', 'gl': 'us', 'hl': 'en'}, serpapi_api_key='75edf52f8da893b36fb5ca9639c81e6edf8b2bc3df0c58d180428ba3bdc8c686', aiosession=None)>)]"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tools.append(math_tool)\n",
        "tools.append(python_tool)"
      ],
      "metadata": {
        "id": "0yJ2_nMk6Xeq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tools"
      ],
      "metadata": {
        "id": "jh_QVyGfHqjZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(prompt.template)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjWtEgNB4SyK",
        "outputId": "a20a3839-b897-45e1-e9df-c3578b0a70a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer the following questions as best you can. You have access to the following tools:\n",
            "\n",
            "{tools}\n",
            "\n",
            "Use the following format:\n",
            "\n",
            "Question: the input question you must answer\n",
            "Thought: you should always think about what to do\n",
            "Action: the action to take, should be one of [{tool_names}]\n",
            "Action Input: the input to the action\n",
            "Observation: the result of the action\n",
            "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
            "Thought: I now know the final answer\n",
            "Final Answer: the final answer to the original input question\n",
            "\n",
            "Begin!\n",
            "\n",
            "Question: {input}\n",
            "Thought:{agent_scratchpad}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uo_2mVrSm3zV"
      },
      "outputs": [],
      "source": [
        "agent = create_react_agent(llm, tools, prompt)\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQQP-n52m9EJ"
      },
      "source": [
        "Дивіться список типів агентів [тут](https://python.langchain.com/v0.1/docs/modules/agents/agent_types/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lklHRp6om-Q0",
        "outputId": "3d0e2ac6-f48f-4562-d1d1-5c963d03f4ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mTo answer the question, I first need to find out who the current leader of China is and then determine their age. After that, I will find the largest prime number smaller than their age.\n",
            "\n",
            "Action: Search  \n",
            "Action Input: \"current leader of China 2023\"  \u001b[0m\u001b[36;1m\u001b[1;3mXi Jinping (born 15 June 1953) is a Chinese politician who has been the general secretary of the Chinese Communist Party (CCP) and chairman of the Central Military Commission (CMC), and thus the paramount leader of China, since 2012. Xi has also been the president of the People's Republic of China (PRC) since 2013.\u001b[0m\u001b[32;1m\u001b[1;3mI have found that the current leader of China is Xi Jinping, who was born on June 15, 1953. Now, I need to calculate his age and then find the largest prime number smaller than that age.\n",
            "\n",
            "Action: python_interpreter  \n",
            "Action Input: \"from datetime import datetime; birth_date = datetime(1953, 6, 15); today = datetime.now(); age = today.year - birth_date.year - ((today.month, today.day) < (birth_date.month, birth_date.day)); print(age)\"  \u001b[0m\u001b[38;5;200m\u001b[1;3m71\n",
            "\u001b[0m\u001b[32;1m\u001b[1;3mI have determined that Xi Jinping is currently 71 years old. Now, I need to find the largest prime number that is smaller than 71.\n",
            "\n",
            "Action: python_interpreter  \n",
            "Action Input: \"def is_prime(n): if n <= 1: return False; for i in range(2, int(n**0.5) + 1): if n % i == 0: return False; return True; largest_prime = None; for num in range(70, 1, -1): if is_prime(num): largest_prime = num; break; print(largest_prime)\"  \u001b[0m\u001b[38;5;200m\u001b[1;3mSyntaxError('invalid syntax', ('<string>', 1, 18, 'def is_prime(n): if n <= 1: return False; for i in range(2, int(n**0.5) + 1): if n % i == 0: return False; return True; largest_prime = None; for num in range(70, 1, -1): if is_prime(num): largest_prime = num; break; print(largest_prime)\\n', 1, 20))\u001b[0m\u001b[32;1m\u001b[1;3mIt seems there was a syntax error in the Python code I provided for finding the largest prime number. I will correct the code and try again.\n",
            "\n",
            "Action: python_interpreter  \n",
            "Action Input: \"def is_prime(n): return n > 1 and all(n % i != 0 for i in range(2, int(n**0.5) + 1)); largest_prime = max(num for num in range(70, 1, -1) if is_prime(num)); print(largest_prime)\"  \u001b[0m\u001b[38;5;200m\u001b[1;3m\u001b[0m\u001b[32;1m\u001b[1;3mIt seems that the output of the last command was not captured. I will run the command again to find the largest prime number smaller than 71.\n",
            "\n",
            "Action: python_interpreter  \n",
            "Action Input: \"def is_prime(n): return n > 1 and all(n % i != 0 for i in range(2, int(n**0.5) + 1)); largest_prime = max(num for num in range(70, 1, -1) if is_prime(num)); print(largest_prime)\"  \u001b[0m\u001b[38;5;200m\u001b[1;3m\u001b[0m\u001b[32;1m\u001b[1;3mIt seems that the output of the last command was not captured again. I will run the command once more to ensure I get the result for the largest prime number smaller than 71.\n",
            "\n",
            "Action: python_interpreter  \n",
            "Action Input: \"def is_prime(n): return n > 1 and all(n % i != 0 for i in range(2, int(n**0.5) + 1)); largest_prime = max(num for num in range(70, 1, -1) if is_prime(num)); print(largest_prime)\"  \u001b[0m\u001b[38;5;200m\u001b[1;3m\u001b[0m\u001b[32;1m\u001b[1;3mIt seems that the output of the previous commands is not being captured. I will try a different approach to find the largest prime number smaller than 71.\n",
            "\n",
            "Action: python_interpreter  \n",
            "Action Input: \"def is_prime(n): return n > 1 and all(n % i != 0 for i in range(2, int(n**0.5) + 1)); primes = [num for num in range(70, 1, -1) if is_prime(num)]; print(primes[0]) if primes else print('No prime found')\"  \u001b[0m\u001b[38;5;200m\u001b[1;3m\u001b[0m\u001b[32;1m\u001b[1;3mIt seems that the output of the previous commands is not being captured. I will try a different approach to find the largest prime number smaller than 71.\n",
            "\n",
            "Action: python_interpreter  \n",
            "Action Input: \"def is_prime(n): return n > 1 and all(n % i != 0 for i in range(2, int(n**0.5) + 1)); largest_prime = None; for num in range(70, 1, -1): if is_prime(num): largest_prime = num; break; print(largest_prime)\"  \u001b[0m\u001b[38;5;200m\u001b[1;3mSyntaxError('invalid syntax', ('<string>', 1, 109, 'def is_prime(n): return n > 1 and all(n % i != 0 for i in range(2, int(n**0.5) + 1)); largest_prime = None; for num in range(70, 1, -1): if is_prime(num): largest_prime = num; break; print(largest_prime)\\n', 1, 112))\u001b[0m\u001b[32;1m\u001b[1;3mIt seems that there is a persistent issue with executing the Python code to find the largest prime number smaller than 71. I will simplify the approach and directly calculate the largest prime number smaller than 71 without using a function.\n",
            "\n",
            "Action: python_interpreter  \n",
            "Action Input: \"for num in range(70, 1, -1): is_prime = all(num % i != 0 for i in range(2, int(num**0.5) + 1)); if is_prime: print(num); break\"  \u001b[0m\u001b[38;5;200m\u001b[1;3mSyntaxError('invalid syntax', ('<string>', 1, 97, 'for num in range(70, 1, -1): is_prime = all(num % i != 0 for i in range(2, int(num**0.5) + 1)); if is_prime: print(num); break\\n', 1, 99))\u001b[0m\u001b[32;1m\u001b[1;3mIt seems that there is a persistent issue with executing the Python code to find the largest prime number smaller than 71. I will simplify the approach and directly calculate the largest prime number smaller than 71 without using a function.\n",
            "\n",
            "Action: python_interpreter  \n",
            "Action Input: \"for num in range(70, 1, -1): if all(num % i != 0 for i in range(2, int(num**0.5) + 1)): print(num); break\"  \u001b[0m\u001b[38;5;200m\u001b[1;3mSyntaxError('invalid syntax', ('<string>', 1, 30, 'for num in range(70, 1, -1): if all(num % i != 0 for i in range(2, int(num**0.5) + 1)): print(num); break\\n', 1, 32))\u001b[0m\u001b[32;1m\u001b[1;3mIt seems that there is a persistent issue with executing the Python code to find the largest prime number smaller than 71. I will try a different approach to find the largest prime number smaller than 71 by using a simpler method.\n",
            "\n",
            "Action: python_interpreter  \n",
            "Action Input: \"def largest_prime_below(n): for num in range(n-1, 1, -1): if all(num % i != 0 for i in range(2, int(num**0.5) + 1)): return num; return None; print(largest_prime_below(71))\"  \u001b[0m\u001b[38;5;200m\u001b[1;3mSyntaxError('invalid syntax', ('<string>', 1, 29, 'def largest_prime_below(n): for num in range(n-1, 1, -1): if all(num % i != 0 for i in range(2, int(num**0.5) + 1)): return num; return None; print(largest_prime_below(71))\\n', 1, 32))\u001b[0m\u001b[32;1m\u001b[1;3mIt seems that there is a persistent issue with executing the Python code to find the largest prime number smaller than 71. I will try a different approach to find the largest prime number smaller than 71 by using a simpler method.\n",
            "\n",
            "Action: python_interpreter  \n",
            "Action Input: \"for num in range(70, 1, -1): if all(num % i != 0 for i in range(2, int(num**0.5) + 1)): print(num); break\"  \u001b[0m\u001b[38;5;200m\u001b[1;3mSyntaxError('invalid syntax', ('<string>', 1, 30, 'for num in range(70, 1, -1): if all(num % i != 0 for i in range(2, int(num**0.5) + 1)): print(num); break\\n', 1, 32))\u001b[0m\u001b[32;1m\u001b[1;3mIt seems that I am encountering persistent issues with executing the Python code to find the largest prime number smaller than 71. Instead of continuing to troubleshoot the code execution, I will manually identify the largest prime number less than 71.\n",
            "\n",
            "The prime numbers less than 71 are: 67, 61, 59, 53, 47, 43, 41, 37, 31, 29, 23, 19, 17, 13, 11, 7, 5, 3, and 2. \n",
            "\n",
            "The largest prime number in this list is 67.\n",
            "\n",
            "Thought: I have determined that the largest prime number smaller than 71 is 67.\n",
            "Final Answer: The current leader of China is Xi Jinping, and the largest prime number smaller than his age (71) is 67.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'Who is the current leader of China? What is the largest prime number that is smaller than their age?',\n",
              " 'output': 'The current leader of China is Xi Jinping, and the largest prime number smaller than his age (71) is 67.'}"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ],
      "source": [
        "agent_executor.invoke({'input': \"Who is the current leader of China? What is the largest prime number that is smaller than their age?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Агенти мають особиливий тип:"
      ],
      "metadata": {
        "id": "-8V-GYOigCl-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "type(agent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "GWSaTSWZf-EH",
        "outputId": "14f472a9-2f16-4969-c7db-b8766ffb83b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "langchain_core.runnables.base.RunnableSequence"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>langchain_core.runnables.base.RunnableSequence</b><br/>def __init__(*steps: RunnableLike, name: Optional[str]=None, first: Optional[Runnable[Any, Any]]=None, middle: Optional[list[Runnable[Any, Any]]]=None, last: Optional[Runnable[Any, Any]]=None) -&gt; None</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py</a>Sequence of Runnables, where the output of each is the input of the next.\n",
              "\n",
              "**RunnableSequence** is the most important composition operator in LangChain\n",
              "as it is used in virtually every chain.\n",
              "\n",
              "A RunnableSequence can be instantiated directly or more commonly by using the `|`\n",
              "operator where either the left or right operands (or both) must be a Runnable.\n",
              "\n",
              "Any RunnableSequence automatically supports sync, async, batch.\n",
              "\n",
              "The default implementations of `batch` and `abatch` utilize threadpools and\n",
              "asyncio gather and will be faster than naive invocation of invoke or ainvoke\n",
              "for IO bound Runnables.\n",
              "\n",
              "Batching is implemented by invoking the batch method on each component of the\n",
              "RunnableSequence in order.\n",
              "\n",
              "A RunnableSequence preserves the streaming properties of its components, so if all\n",
              "components of the sequence implement a `transform` method -- which\n",
              "is the method that implements the logic to map a streaming input to a streaming\n",
              "output -- then the sequence will be able to stream input to output!\n",
              "\n",
              "If any component of the sequence does not implement transform then the\n",
              "streaming will only begin after this component is run. If there are\n",
              "multiple blocking components, streaming begins after the last one.\n",
              "\n",
              "Please note: RunnableLambdas do not support `transform` by default! So if\n",
              "    you need to use a RunnableLambdas be careful about where you place them in a\n",
              "    RunnableSequence (if you need to use the .stream()/.astream() methods).\n",
              "\n",
              "    If you need arbitrary logic and need streaming, you can subclass\n",
              "    Runnable, and implement `transform` for whatever logic you need.\n",
              "\n",
              "Here is a simple example that uses simple functions to illustrate the use of\n",
              "RunnableSequence:\n",
              "\n",
              "    .. code-block:: python\n",
              "\n",
              "        from langchain_core.runnables import RunnableLambda\n",
              "\n",
              "        def add_one(x: int) -&gt; int:\n",
              "            return x + 1\n",
              "\n",
              "        def mul_two(x: int) -&gt; int:\n",
              "            return x * 2\n",
              "\n",
              "        runnable_1 = RunnableLambda(add_one)\n",
              "        runnable_2 = RunnableLambda(mul_two)\n",
              "        sequence = runnable_1 | runnable_2\n",
              "        # Or equivalently:\n",
              "        # sequence = RunnableSequence(first=runnable_1, last=runnable_2)\n",
              "        sequence.invoke(1)\n",
              "        await sequence.ainvoke(1)\n",
              "\n",
              "        sequence.batch([1, 2, 3])\n",
              "        await sequence.abatch([1, 2, 3])\n",
              "\n",
              "Here&#x27;s an example that uses streams JSON output generated by an LLM:\n",
              "\n",
              "    .. code-block:: python\n",
              "\n",
              "        from langchain_core.output_parsers.json import SimpleJsonOutputParser\n",
              "        from langchain_openai import ChatOpenAI\n",
              "\n",
              "        prompt = PromptTemplate.from_template(\n",
              "            &#x27;In JSON format, give me a list of {topic} and their &#x27;\n",
              "            &#x27;corresponding names in French, Spanish and in a &#x27;\n",
              "            &#x27;Cat Language.&#x27;\n",
              "        )\n",
              "\n",
              "        model = ChatOpenAI()\n",
              "        chain = prompt | model | SimpleJsonOutputParser()\n",
              "\n",
              "        async for chunk in chain.astream({&#x27;topic&#x27;: &#x27;colors&#x27;}):\n",
              "            print(&#x27;-&#x27;)  # noqa: T201\n",
              "            print(chunk, sep=&#x27;&#x27;, flush=True)  # noqa: T201</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 2659);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Він створюється за допомогою RunnableAssign, PromptTemplate, RunnableBinding та ReActSingleInputOutputParser, структурованих у формі [LangChain Expression Language (LCEL)](https://python.langchain.com/docs/concepts/#langchain-expression-language-lcel)"
      ],
      "metadata": {
        "id": "uJiVsBKsg01o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent"
      ],
      "metadata": {
        "id": "oGUCf13cghhl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Перший крок — це RunnableAssign, який відповідає за присвоєння пар \"ключ-значення\" вхідним даним формату Dict[str, Any]. У цьому випадку ключем є agent_scratchpad, а значенням є RunnableLambda, що перетворює intermediate_steps на рядок."
      ],
      "metadata": {
        "id": "sTCHfhigg-EZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent.steps[1].partial_variables"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_xmX2HPglr0",
        "outputId": "5e3de9d2-2eec-4085-e104-d0fca6a4280e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'tools': \"Search(query: str, **kwargs: Any) -> str - A search engine. Useful for when you need to answer questions about current events. Input should be a search query.\\nCalculator(*args: Any, callbacks: Union[list[langchain_core.callbacks.base.BaseCallbackHandler], langchain_core.callbacks.base.BaseCallbackManager, NoneType] = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, **kwargs: Any) -> Any - Useful for when you need to answer questions about math. This tool is only for math questions and nothing else. Only input math expressions.\\npython_interpreter(command: str, timeout: Optional[int] = None) -> str - A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you necessarily should print it out with `print(...)`. Otherwise you won't see the result! It's very important.\",\n",
              " 'tool_names': 'Search, Calculator, python_interpreter'}"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ми ще і промпти на кожному етапі можемо переглянути."
      ],
      "metadata": {
        "id": "IKOV10C7hwBP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(agent.steps[1].template)"
      ],
      "metadata": {
        "id": "mvOS-xqGhuIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vV7EY8hirPoR"
      },
      "source": [
        "### Пам'ять\n",
        "Часто нам хочеться, аби LLM памʼятала історію діалогу. Найпростіша форма пам'яті - це просто передача повідомлень історії чату по ланцюжку. Ось приклад:\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        SystemMessage(\n",
        "            content=\"You are a helpful assistant. Answer all questions to the best of your ability.\"\n",
        "        ),\n",
        "        MessagesPlaceholder(variable_name=\"messages\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain = prompt | llm\n",
        "\n",
        "ai_msg = chain.invoke(\n",
        "    {\n",
        "        \"messages\": [\n",
        "            HumanMessage(\n",
        "                content=\"Translate from English to French: I love programming.\"\n",
        "            ),\n",
        "            AIMessage(content=\"J'adore la programmation.\"),\n",
        "            HumanMessage(content=\"What did you just say?\"),\n",
        "        ],\n",
        "    }\n",
        ")\n",
        "print(ai_msg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YwH30dsplv1Z",
        "outputId": "6958d99b-7def-415a-f183-4fa5b26d6dd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='I translated \"I love programming\" into French, which is \"J\\'adore la programmation.\"' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 56, 'total_tokens': 74, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_7693ae462b', 'finish_reason': 'stop', 'logprobs': None} id='run-2c72d6a4-5e33-4b14-bb83-6ea8f1e13a96-0' usage_metadata={'input_tokens': 56, 'output_tokens': 18, 'total_tokens': 74, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ai_msg.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "A0lppeeUMNdk",
        "outputId": "5cffef48-e833-41b6-ce22-d883d6dc2adc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I translated \"I love programming\" into French, which is \"J\\'adore la programmation.\"'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ми бачимо, що, передаючи попередню розмову по ланцюжку, він може використовувати її як контекст для відповідей на запитання. Це основна концепція, що лежить в основі пам'яті чат-ботів - решта посібника продемонструє зручні прийоми для передачі або переформатування повідомлень.\n"
      ],
      "metadata": {
        "id": "13zlMwv2n1zL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Автоматичне керування історією\n",
        "\n",
        "У попередніх прикладах повідомлення передаються в ланцюжок (та модель) явно. Це абсолютно прийнятний підхід, але він вимагає зовнішнього керування новими повідомленнями. LangChain також надає спосіб створення застосунків, що мають пам'ять, використовуючи [persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/) LangGraph. Ви можете увімкнути persistence у застосунках LangGraph, надавши контрольну точку (checkpointer) під час компіляції графа.\n",
        "\n"
      ],
      "metadata": {
        "id": "Y9za9DEdnp7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -q langgraph"
      ],
      "metadata": {
        "id": "OA0XYEuAm_3A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ed77a01-7f4e-4541-dba0-b0867b2d5444"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/113.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m112.6/113.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.5/113.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.graph import START, MessagesState, StateGraph\n",
        "\n",
        "workflow = StateGraph(state_schema=MessagesState)\n",
        "\n",
        "# Визначаємо функцію, яка викликає модель\n",
        "def call_model(state: MessagesState):\n",
        "    system_prompt = (\n",
        "        \"You are a helpful assistant. \"\n",
        "        \"Answer all questions to the best of your ability.\"\n",
        "    )\n",
        "    messages = [SystemMessage(content=system_prompt)] + state[\"messages\"]\n",
        "    response = llm.invoke(messages)\n",
        "    return {\"messages\": response}\n",
        "\n",
        "\n",
        "# Визначаємо вузол та ребро\n",
        "workflow.add_node(\"model\", call_model)\n",
        "workflow.add_edge(START, \"model\")\n",
        "\n",
        "# Додаємо простий контроль стану в пам'яті\n",
        "memory = MemorySaver()\n",
        "app = workflow.compile(checkpointer=memory)"
      ],
      "metadata": {
        "id": "Lien7znQm6dG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "app.invoke(\n",
        "    {\"messages\": [HumanMessage(content=\"Translate to French: I love programming.\")]},\n",
        "    config={\"configurable\": {\"thread_id\": \"1\"}},\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZjnkTgYnN6i",
        "outputId": "18c39b98-325d-4b2d-f475-9721b9ca6cb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'messages': [HumanMessage(content='Translate to French: I love programming.', additional_kwargs={}, response_metadata={}, id='21ec9a11-0de2-4c97-9a81-9b8f06b8091c'),\n",
              "  AIMessage(content=\"J'adore programmer.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 4, 'prompt_tokens': 35, 'total_tokens': 39, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_482c22a7bc', 'finish_reason': 'stop', 'logprobs': None}, id='run-7a7bd92b-63bc-4059-a542-9dc2668f7489-0', usage_metadata={'input_tokens': 35, 'output_tokens': 4, 'total_tokens': 39, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})]}"
            ]
          },
          "metadata": {},
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "app.invoke(\n",
        "    {\"messages\": [HumanMessage(content=\"What did I just ask you?\")]},\n",
        "    config={\"configurable\": {\"thread_id\": \"1\"}},\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWrLOQrwnVrs",
        "outputId": "28ec3d6f-0017-4b44-bea2-9c6114db69ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'messages': [HumanMessage(content='Translate to French: I love programming.', additional_kwargs={}, response_metadata={}, id='21ec9a11-0de2-4c97-9a81-9b8f06b8091c'),\n",
              "  AIMessage(content=\"J'adore programmer.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 4, 'prompt_tokens': 35, 'total_tokens': 39, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_482c22a7bc', 'finish_reason': 'stop', 'logprobs': None}, id='run-7a7bd92b-63bc-4059-a542-9dc2668f7489-0', usage_metadata={'input_tokens': 35, 'output_tokens': 4, 'total_tokens': 39, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}}),\n",
              "  HumanMessage(content='What did I just ask you?', additional_kwargs={}, response_metadata={}, id='01b3f193-5f40-4103-a639-093e4149ff1c'),\n",
              "  AIMessage(content='You asked me to translate \"I love programming\" into French.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 54, 'total_tokens': 67, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_482c22a7bc', 'finish_reason': 'stop', 'logprobs': None}, id='run-e4bf6029-1d77-4b19-848f-129d4ca827a8-0', usage_metadata={'input_tokens': 54, 'output_tokens': 13, 'total_tokens': 67, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})]}"
            ]
          },
          "metadata": {},
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Зміна історії чату\n",
        "Зміна збережених повідомлень чату може допомогти вашому чат-боту впоратися з різними ситуаціями.\n",
        "\n",
        "## Обрізання повідомлень\n",
        "LLM і чат-моделі мають обмежені контекстні вікна, і навіть якщо ви не перевищуєте ліміти, ви можете обмежити кількість відволікаючих чинників, з якими доводиться мати справу моделі. Одне з рішень - обрізати повідомлення історії перед тим, як передавати їх моделі. Давайте розглянемо приклад історії з додатком, який ми оголосили вище:\n"
      ],
      "metadata": {
        "id": "Om0RC09emcv3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "demo_ephemeral_chat_history = [\n",
        "    HumanMessage(content=\"Hey there! I'm Nemo.\"),\n",
        "    AIMessage(content=\"Hello!\"),\n",
        "    HumanMessage(content=\"How are you today?\"),\n",
        "    AIMessage(content=\"Fine thanks!\"),\n",
        "]\n",
        "\n",
        "app.invoke(\n",
        "    {\n",
        "        \"messages\": demo_ephemeral_chat_history\n",
        "        + [HumanMessage(content=\"What's my name?\")]\n",
        "    },\n",
        "    config={\"configurable\": {\"thread_id\": \"2\"}},\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aA6JPbF5od3W",
        "outputId": "fb70c981-b6ae-41ce-bc00-7371cf1dc5a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'messages': [HumanMessage(content=\"Hey there! I'm Nemo.\", additional_kwargs={}, response_metadata={}, id='e5054b65-d7ee-477a-9519-027e923206fb'),\n",
              "  AIMessage(content='Hello!', additional_kwargs={}, response_metadata={}, id='0d22c145-cc6d-4cea-a492-3318001cf2c8'),\n",
              "  HumanMessage(content='How are you today?', additional_kwargs={}, response_metadata={}, id='6e6ca81c-6c4e-4d8f-a5e2-9377459909bd'),\n",
              "  AIMessage(content='Fine thanks!', additional_kwargs={}, response_metadata={}, id='e71319c0-aefb-4858-bb33-91e1452289d1'),\n",
              "  HumanMessage(content=\"What's my name?\", additional_kwargs={}, response_metadata={}, id='19ee0ae3-fc42-486c-863a-669701ac6c89'),\n",
              "  AIMessage(content='Your name is Nemo!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 63, 'total_tokens': 68, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_482c22a7bc', 'finish_reason': 'stop', 'logprobs': None}, id='run-16f9babc-a91b-45e8-9750-9527af311545-0', usage_metadata={'input_tokens': 63, 'output_tokens': 5, 'total_tokens': 68, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})]}"
            ]
          },
          "metadata": {},
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ми бачимо, що програма запам'ятовує попередньо завантажене ім'я.\n",
        "\n",
        "Але уявімо, що у нас дуже маленьке контекстне вікно, і ми хочемо зменшити кількість повідомлень, що передаються моделі, до 2 останніх. Ми можемо використати вбудовану утиліту trim_messages для відсікання повідомлень на основі їх кількості токенів до того, як вони досягнуть нашого запиту. У цьому випадку ми будемо вважати кожне повідомлення за 1 «токен» і залишимо лише два останніх повідомлення:\n",
        "\n"
      ],
      "metadata": {
        "id": "snHoQ3vUorK8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import trim_messages\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.graph import START, MessagesState, StateGraph\n",
        "\n",
        "# Визначаємо функцію обрізання повідомлень\n",
        "# рахуємо кожне повідомлення як 1 \"токен\" (token_counter=len) і залишаємо лише останні два повідомлення\n",
        "trimmer = trim_messages(strategy=\"last\", max_tokens=2, token_counter=len)\n",
        "\n",
        "workflow = StateGraph(state_schema=MessagesState)\n",
        "\n",
        "# Визначаємо функцію, яка викликає модель\n",
        "def call_model(state: MessagesState):\n",
        "    trimmed_messages = trimmer.invoke(state[\"messages\"])\n",
        "    system_prompt = (\n",
        "        \"You are a helpful assistant. \"\n",
        "        \"Answer all questions to the best of your ability.\"\n",
        "    )\n",
        "    messages = [SystemMessage(content=system_prompt)] + trimmed_messages\n",
        "    response = llm.invoke(messages)\n",
        "    return {\"messages\": response}\n",
        "\n",
        "\n",
        "# Визначаємо вузол та ребро\n",
        "workflow.add_node(\"model\", call_model)\n",
        "workflow.add_edge(START, \"model\")\n",
        "\n",
        "# Додаємо простий контроль стану в пам'яті\n",
        "memory = MemorySaver()\n",
        "app = workflow.compile(checkpointer=memory)\n"
      ],
      "metadata": {
        "id": "3kGUgd-bot7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "app.invoke(\n",
        "    {\n",
        "        \"messages\": demo_ephemeral_chat_history\n",
        "        + [HumanMessage(content=\"What is my name?\")]\n",
        "    },\n",
        "    config={\"configurable\": {\"thread_id\": \"3\"}},\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_StTuT13o2VA",
        "outputId": "22574193-14a1-4070-8f22-f58e7cdb97cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'messages': [HumanMessage(content=\"Hey there! I'm Nemo.\", additional_kwargs={}, response_metadata={}, id='e5054b65-d7ee-477a-9519-027e923206fb'),\n",
              "  AIMessage(content='Hello!', additional_kwargs={}, response_metadata={}, id='0d22c145-cc6d-4cea-a492-3318001cf2c8'),\n",
              "  HumanMessage(content='How are you today?', additional_kwargs={}, response_metadata={}, id='6e6ca81c-6c4e-4d8f-a5e2-9377459909bd'),\n",
              "  AIMessage(content='Fine thanks!', additional_kwargs={}, response_metadata={}, id='e71319c0-aefb-4858-bb33-91e1452289d1'),\n",
              "  HumanMessage(content='What is my name?', additional_kwargs={}, response_metadata={}, id='a45d3ebd-247d-4191-ae02-7cb01281aa71'),\n",
              "  AIMessage(content=\"I'm sorry, but I don't have access to personal information about users unless you provide it. If you'd like, you can tell me your name!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 29, 'prompt_tokens': 39, 'total_tokens': 68, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_482c22a7bc', 'finish_reason': 'stop', 'logprobs': None}, id='run-1173537a-df50-401a-8955-b244f8a6631e-0', usage_metadata={'input_tokens': 39, 'output_tokens': 29, 'total_tokens': 68, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})]}"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Саммарі історії повідомлень\n",
        "\n",
        "Ми можемо використовувати цей промпт і в інших випадках. Наприклад, ми можемо використовувати додатковий виклик LLM для створення резюме розмови перед викликом нашого додатку. Давайте відтворимо нашу історію чату:\n"
      ],
      "metadata": {
        "id": "Ao4sO6JdpXbq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "demo_ephemeral_chat_history = [\n",
        "    HumanMessage(content=\"Hey there! I'm Nemo.\"),\n",
        "    AIMessage(content=\"Hello!\"),\n",
        "    HumanMessage(content=\"How are you today?\"),\n",
        "    AIMessage(content=\"Fine thanks!\"),\n",
        "]"
      ],
      "metadata": {
        "id": "kf38KDEspg7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage, RemoveMessage\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.graph import START, MessagesState, StateGraph\n",
        "\n",
        "workflow = StateGraph(state_schema=MessagesState)\n",
        "\n",
        "\n",
        "# Функція, яка викликає модель\n",
        "def call_model(state: MessagesState):\n",
        "    system_prompt = (\n",
        "        \"You are a helpful assistant. \"\n",
        "        \"Answer all questions to the best of your ability. \"\n",
        "        \"The provided chat history includes a summary of the earlier conversation.\"\n",
        "    )\n",
        "    system_message = SystemMessage(content=system_prompt)\n",
        "    message_history = state[\"messages\"][:-1]  # вилучаємо найостанніше введення\n",
        "    # Підсумовуємо повідомлення, якщо історія чату досягає певного розміру\n",
        "    if len(message_history) >= 4:\n",
        "        last_human_message = state[\"messages\"][-1]\n",
        "        # Викликаємо модель для створення підсумкових повідомлень розмови\n",
        "        summary_prompt = (\n",
        "            \"Distill the above chat messages into a single summary message. \"\n",
        "            \"Include as many specific details as you can.\"\n",
        "        )\n",
        "        summary_message = llm.invoke(\n",
        "            message_history + [HumanMessage(content=summary_prompt)]\n",
        "        )\n",
        "\n",
        "        # Видаляємо повідомлення, які більше не хочемо відображати\n",
        "        delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"]]\n",
        "        # Повторно додаємо повідомлення від користувача\n",
        "        human_message = HumanMessage(content=last_human_message.content)\n",
        "        # Викликаємо модель для обробки підсумкових повідомлень і відповіді\n",
        "        response = llm.invoke([system_message, summary_message, human_message])\n",
        "        message_updates = [summary_message, human_message, response] + delete_messages\n",
        "    else:\n",
        "        message_updates = llm.invoke([system_message] + state[\"messages\"])\n",
        "\n",
        "    return {\"messages\": message_updates}\n",
        "\n",
        "\n",
        "# Визначаємо вузол та ребро\n",
        "workflow.add_node(\"model\", call_model)\n",
        "workflow.add_edge(START, \"model\")\n",
        "\n",
        "# Додаємо простий контроль стану в пам'яті\n",
        "memory = MemorySaver()\n",
        "app = workflow.compile(checkpointer=memory)\n"
      ],
      "metadata": {
        "id": "cWhEwvExpjAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "app.invoke(\n",
        "    {\n",
        "        \"messages\": demo_ephemeral_chat_history\n",
        "        + [HumanMessage(\"What did I say my name was?\")]\n",
        "    },\n",
        "    config={\"configurable\": {\"thread_id\": \"4\"}},\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BhvmCM0_ppdE",
        "outputId": "c212c32a-8439-4d21-a7dc-f17f53f822e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'messages': [AIMessage(content='Nemo greeted me with \"Hey there!\" and asked how I was doing, to which I responded that I was fine.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 60, 'total_tokens': 85, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_482c22a7bc', 'finish_reason': 'stop', 'logprobs': None}, id='run-df5ffc76-77c8-4a2a-923b-31edac843e88-0', usage_metadata={'input_tokens': 60, 'output_tokens': 25, 'total_tokens': 85, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}}),\n",
              "  HumanMessage(content='What did I say my name was?', additional_kwargs={}, response_metadata={}, id='e160fac5-f4a2-4dca-92e2-54aa32fe8c52'),\n",
              "  AIMessage(content='You mentioned that your name is Nemo.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 76, 'total_tokens': 84, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_482c22a7bc', 'finish_reason': 'stop', 'logprobs': None}, id='run-0585565c-906b-4056-a7c9-4d7294e44f79-0', usage_metadata={'input_tokens': 76, 'output_tokens': 8, 'total_tokens': 84, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})]}"
            ]
          },
          "metadata": {},
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Зауважте, що повторний запуск програми продовжить накопичення історії доти, доки вона не досягне вказаної кількості повідомлень (у нашому випадку - чотирьох). У цей момент ми створимо ще один звіт, згенерований з початкового звіту плюс нові повідомлення, і так далі.\n"
      ],
      "metadata": {
        "id": "YvdI579VpvOJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e--hMIfWIwsj"
      },
      "source": [
        "# Порівняння та оцінка LLM (Large Language Models)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Зручно мати механізм порівняти кілька LLM. Для порівняння використаємо моделі з HuggingFace. Всі моделі можна знайти [тут](https://huggingface.co/models)\n",
        "\n",
        "**Hugging Face** — це платформа та компанія, яка спеціалізується на штучному інтелекті, зокрема на роботі з мовними моделями та NLP (обробкою природної мови). Вони створили популярну бібліотеку **Transformers**, яка надає інструменти для роботи з передовими моделями штучного інтелекту, такими як GPT, BERT, T5, та інші. Hugging Face також має платформу для спільного використання моделей, де розробники можуть завантажувати, тестувати та використовувати попередньо навчені моделі для різних задач, таких як текстова класифікація, генерація тексту та переклад."
      ],
      "metadata": {
        "id": "OBF8wW_TslU1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -q langchain-huggingface"
      ],
      "metadata": {
        "id": "yCB5frdJp7G6",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dNA4TsHpu6OM"
      },
      "outputs": [],
      "source": [
        "os.environ[\"HF_TOKEN\"] = creds['HUGGINGFACEHUB_API_TOKEN']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lgesD0jrvDyG"
      },
      "outputs": [],
      "source": [
        "from langchain_huggingface import HuggingFaceEndpoint"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "overal_temperature = 0.1\n",
        "\n",
        "mistral = HuggingFaceEndpoint(\n",
        "    repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
        "    temperature=overal_temperature,\n",
        "    max_new_tokens=200\n",
        ")\n",
        "\n",
        "gemma = HuggingFaceEndpoint(\n",
        "    repo_id=\"google/gemma-7b\",\n",
        "    temperature=overal_temperature,\n",
        "    max_new_tokens=500\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BvX1v6yAqUfp",
        "outputId": "a9297952-492d-4f59-8e5d-2240c0c4e9bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n",
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDWW8nDERcGU"
      },
      "source": [
        "## Налаштування лабораторії для порівняння"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sy4s37W9m7X6"
      },
      "outputs": [],
      "source": [
        "from langchain.model_laboratory import ModelLaboratory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZUMGKuvn_HV"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: Let's think step by step.\"\"\"\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5jHtwgkabw3"
      },
      "outputs": [],
      "source": [
        "models_list = [\n",
        "    mistral,\n",
        "    gemma,\n",
        "    llm\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9s07AIuJm_Gv"
      },
      "outputs": [],
      "source": [
        "lab = ModelLaboratory.from_llms(models_list, prompt=prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDQ8VcedrkIw"
      },
      "source": [
        "Давайте запустимо це на деяких і порівняємо!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Eb6pNpimfu6",
        "outputId": "2ac4f0ce-d045-4211-d34f-5997710a2ce3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mInput:\u001b[0m\n",
            "What is the opposite of up?\n",
            "\n",
            "\u001b[1mHuggingFaceEndpoint\u001b[0m\n",
            "Params: {'endpoint_url': None, 'task': None, 'model_kwargs': {}}\n",
            "\u001b[36;1m\u001b[1;3m The opposite of 'up' would be a direction that is lower than the current position. So, the opposite of 'up' is 'down'.\u001b[0m\n",
            "\n",
            "\u001b[1mHuggingFaceEndpoint\u001b[0m\n",
            "Params: {'endpoint_url': None, 'task': None, 'model_kwargs': {}}\n",
            "\u001b[33;1m\u001b[1;3m\n",
            "\n",
            "Up is a vertical direction.\n",
            "\n",
            "Down is the opposite of up.\n",
            "\n",
            "Down is a vertical direction.\n",
            "\n",
            "Therefore, the opposite of up is down.\u001b[0m\n",
            "\n",
            "client=<openai.resources.chat.completions.Completions object at 0x7864fc008c10> async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7864fbed0160> root_client=<openai.OpenAI object at 0x7864fc008100> root_async_client=<openai.AsyncOpenAI object at 0x7864fc008b80> model_name='gpt-4o-mini' model_kwargs={} openai_api_key=SecretStr('**********')\n",
            "\u001b[38;5;200m\u001b[1;3mTo determine the opposite of \"up,\" we can break it down as follows:\n",
            "\n",
            "1. **Understanding \"Up\":** The term \"up\" refers to a direction that is higher or elevated relative to a point of reference, such as the ground or a surface.\n",
            "\n",
            "2. **Identifying the Opposite Direction:** To find the opposite, we consider the direction that is lower or descending relative to the same point of reference.\n",
            "\n",
            "3. **Conclusion:** The opposite of \"up\" is \"down.\"\n",
            "\n",
            "So, step by step, we arrive at the answer: the opposite of up is down.\u001b[0m\n",
            "\n"
          ]
        }
      ],
      "source": [
        "lab.compare(\"What is the opposite of up?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "puWRd2nwT5eD",
        "outputId": "6ae926f2-cef9-461d-b6bb-4571169566dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mInput:\u001b[0m\n",
            "Answer the following question by reasoning step by step. The cafeteria had 23 apples. If they used 20 for lunch, and bought 6 more, how many apple do they have?\n",
            "\n",
            "\u001b[1mHuggingFaceEndpoint\u001b[0m\n",
            "Params: {'endpoint_url': None, 'task': None, 'model_kwargs': {}}\n",
            "\u001b[36;1m\u001b[1;3m\n",
            "\n",
            "1. The cafeteria started with 23 apples.\n",
            "2. They used 20 apples for lunch, so we subtract 20 from the initial number of apples: 23 - 20 = 3.\n",
            "3. They bought 6 more apples, so we add these to the remaining apples: 3 + 6 = 9.\n",
            "\n",
            "Therefore, the cafeteria now has 9 apples.\u001b[0m\n",
            "\n",
            "\u001b[1mHuggingFaceEndpoint\u001b[0m\n",
            "Params: {'endpoint_url': None, 'task': None, 'model_kwargs': {}}\n",
            "\u001b[33;1m\u001b[1;3m\n",
            "\n",
            "Step 1/2\n",
            "Step 1: We know that the cafeteria had 23 apples.\n",
            "\n",
            "Step 2/2\n",
            "Step 2: They used 20 apples for lunch, so they have 3 apples left. Step 3: They bought 6 more apples, so they now have 3+6=9 apples. Therefore, the cafeteria has 9 apples.\u001b[0m\n",
            "\n",
            "client=<openai.resources.chat.completions.Completions object at 0x7864fc008c10> async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7864fbed0160> root_client=<openai.OpenAI object at 0x7864fc008100> root_async_client=<openai.AsyncOpenAI object at 0x7864fc008b80> model_name='gpt-4o-mini' model_kwargs={} openai_api_key=SecretStr('**********')\n",
            "\u001b[38;5;200m\u001b[1;3mSure! Let's break it down step by step:\n",
            "\n",
            "1. **Starting Amount of Apples**: The cafeteria initially has 23 apples.\n",
            "\n",
            "2. **Apples Used for Lunch**: They used 20 apples for lunch. To find out how many apples they have left after using some, we subtract the number of apples used from the initial amount:\n",
            "   \\[\n",
            "   23 - 20 = 3\n",
            "   \\]\n",
            "   So, after using 20 apples, the cafeteria has 3 apples left.\n",
            "\n",
            "3. **Apples Bought**: The cafeteria bought 6 more apples. Now we need to add those apples to the remaining amount:\n",
            "   \\[\n",
            "   3 + 6 = 9\n",
            "   \\]\n",
            "   Therefore, after buying 6 more apples, the cafeteria now has 9 apples.\n",
            "\n",
            "4. **Final Count of Apples**: The final count of apples in the cafeteria is 9.\n",
            "\n",
            "So, the cafeteria has **9 apples**.\u001b[0m\n",
            "\n"
          ]
        }
      ],
      "source": [
        "lab.compare(\"Answer the following question by reasoning step by step. The cafeteria had 23 apples. \\\n",
        "If they used 20 for lunch, and bought 6 more, how many apple do they have?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AP63DmDbaY_X",
        "outputId": "7d476cc0-e6f2-41e0-84f7-2e25d3f4df56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mInput:\u001b[0m\n",
            "\n",
            "  Can Elon Musk have a conversation with George Washington? Give the rationale before answering.\n",
            "\n",
            "\n",
            "\u001b[1mHuggingFaceEndpoint\u001b[0m\n",
            "Params: {'endpoint_url': None, 'task': None, 'model_kwargs': {}}\n",
            "\u001b[36;1m\u001b[1;3m Elon Musk is a contemporary figure, alive in the present day. George Washington, on the other hand, lived from 1732 to 1799, making him a historical figure.\n",
            "\n",
            "The ability to have a conversation between two individuals, one living and one historical, requires a means of communication that transcends time and space. Currently, there is no scientifically proven method for achieving this. Therefore, Elon Musk cannot have a conversation with George Washington based on our current understanding of reality and technology.\u001b[0m\n",
            "\n",
            "\u001b[1mHuggingFaceEndpoint\u001b[0m\n",
            "Params: {'endpoint_url': None, 'task': None, 'model_kwargs': {}}\n",
            "\u001b[33;1m\u001b[1;3m\n",
            "\n",
            "1. Can George Washington have a conversation with Elon Musk?\n",
            "\n",
            "2. Can George Washington have a conversation with anyone?\n",
            "\n",
            "3. Can anyone have a conversation with Elon Musk?\n",
            "\n",
            "4. Can anyone have a conversation with anyone?\n",
            "\n",
            "5. Can anyone have a conversation with anyone?\n",
            "\n",
            "6. Can anyone have a conversation with anyone?\n",
            "\n",
            "7. Can anyone have a conversation with anyone?\n",
            "\n",
            "8. Can anyone have a conversation with anyone?\n",
            "\n",
            "9. Can anyone have a conversation with anyone?\n",
            "\n",
            "10. Can anyone have a conversation with anyone?\n",
            "\n",
            "11. Can anyone have a conversation with anyone?\n",
            "\n",
            "12. Can anyone have a conversation with anyone?\n",
            "\n",
            "13. Can anyone have a conversation with anyone?\n",
            "\n",
            "14. Can anyone have a conversation with anyone?\n",
            "\n",
            "15. Can anyone have a conversation with anyone?\n",
            "\n",
            "16. Can anyone have a conversation with anyone?\n",
            "\n",
            "17. Can anyone have a conversation with anyone?\n",
            "\n",
            "18. Can anyone have a conversation with anyone?\n",
            "\n",
            "19. Can anyone have a conversation with anyone?\n",
            "\n",
            "20. Can anyone have a conversation with anyone?\n",
            "\n",
            "21. Can anyone have a conversation with anyone?\n",
            "\n",
            "22. Can anyone have a conversation with anyone?\n",
            "\n",
            "23. Can anyone have a conversation with anyone?\n",
            "\n",
            "24. Can anyone have a conversation with anyone?\n",
            "\n",
            "25. Can anyone have a conversation with anyone?\n",
            "\n",
            "26. Can anyone have a conversation with anyone?\n",
            "\n",
            "27. Can anyone have a conversation with anyone?\n",
            "\n",
            "28. Can anyone have a conversation with anyone?\n",
            "\n",
            "29. Can anyone have a conversation with anyone?\n",
            "\n",
            "30. Can anyone have a conversation with anyone?\n",
            "\n",
            "31. Can anyone have a conversation with anyone?\n",
            "\n",
            "32. Can anyone have a conversation with anyone?\n",
            "\n",
            "33. Can anyone have a conversation with anyone?\n",
            "\n",
            "34. Can anyone have a conversation with anyone?\n",
            "\n",
            "35. Can anyone have a conversation with anyone?\n",
            "\n",
            "36. Can anyone have a conversation with anyone?\n",
            "\n",
            "37. Can anyone have a conversation with anyone?\n",
            "\n",
            "38. Can anyone have a conversation with anyone?\n",
            "\n",
            "39. Can anyone have a conversation with anyone?\n",
            "\n",
            "40. Can anyone have a conversation with anyone?\n",
            "\n",
            "41. Can anyone have a conversation with anyone?\n",
            "\n",
            "42. Can anyone have a conversation with anyone?\n",
            "\n",
            "\u001b[0m\n",
            "\n",
            "client=<openai.resources.chat.completions.Completions object at 0x7864fc008c10> async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7864fbed0160> root_client=<openai.OpenAI object at 0x7864fc008100> root_async_client=<openai.AsyncOpenAI object at 0x7864fc008b80> model_name='gpt-4o-mini' model_kwargs={} openai_api_key=SecretStr('**********')\n",
            "\u001b[38;5;200m\u001b[1;3mTo determine whether Elon Musk can have a conversation with George Washington, we need to consider several factors:\n",
            "\n",
            "1. **Time Period**: George Washington lived from 1732 to 1799, while Elon Musk was born in 1971. They exist in entirely different time periods.\n",
            "\n",
            "2. **Medium of Communication**: Conversations typically involve real-time interaction, which is not possible across different eras without some form of time travel or advanced technology that allows for such interactions.\n",
            "\n",
            "3. **Technological Limitations**: Even if we were to hypothetically consider a scenario where time travel or advanced communication technology exists, we would need to address whether the two individuals could understand each other given the differences in language, culture, and societal context.\n",
            "\n",
            "4. **Reality vs. Fiction**: In fiction, characters can interact across time periods, but this does not reflect the real world. In reality, such a conversation is impossible.\n",
            "\n",
            "Based on these considerations, the answer is: **No, Elon Musk cannot have a conversation with George Washington.** The barriers of time, communication, and reality make it impossible for the two to interact directly.\u001b[0m\n",
            "\n"
          ]
        }
      ],
      "source": [
        "lab.compare('''\n",
        "  Can Elon Musk have a conversation with George Washington? Give the rationale before answering.\n",
        "''')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbqm52kHarID"
      },
      "source": [
        "Давайте змінимо запит."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uoq5C6eMT5jr"
      },
      "outputs": [],
      "source": [
        "template = \"\"\"You are a professional social media manager who can write great posts in linkedin to increase appeal of persons profile: {request}\n",
        "\n",
        "Story:\"\"\"\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"request\"])\n",
        "\n",
        "lab = ModelLaboratory.from_llms(models_list, prompt=prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lo1mo_8OT5nI",
        "outputId": "1630bd80-523a-4b3c-be96-20be268c6477"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mInput:\u001b[0m\n",
            "I have passed a course in large language models (1 month duration). Write a post about that.\n",
            "\n",
            "\u001b[1mHuggingFaceEndpoint\u001b[0m\n",
            "Params: {'endpoint_url': None, 'task': None, 'model_kwargs': {}}\n",
            "\u001b[36;1m\u001b[1;3m You've recently completed a one-month intensive course in Large Language Models. You're excited about the new skills you've gained and the potential they have to enhance your professional profile. In this post, you'll share your experience and the key takeaways from the course.\n",
            "\n",
            "Post: 📣 Exciting news! I've just completed a one-month intensive course in Large Language Models, and I'm thrilled to share my experience and the key takeaways with all of you.\n",
            "\n",
            "💡 Large Language Models are artificial intelligence systems that can process and generate human-like text. They're used in various applications, from chatbots and customer service to content creation and translation.\n",
            "\n",
            "🌟 The course covered the fundamentals of Large Language Models, including their architecture, training, and applications. I learned how to build and fine-tune models using popular framework\u001b[0m\n",
            "\n",
            "\u001b[1mHuggingFaceEndpoint\u001b[0m\n",
            "Params: {'endpoint_url': None, 'task': None, 'model_kwargs': {}}\n",
            "\u001b[33;1m\u001b[1;3m\n",
            "\n",
            "I am a social media manager who has been working in the field for several years. I have a lot of experience in writing posts that are both interesting and informative. I have also taken a course on large language models, which has given me a better understanding of how to write posts that will be more appealing to my audience.\n",
            "\n",
            "In this post, I will be discussing the benefits of taking a course on large language models. I will also be providing some tips on how to write posts that are both interesting and informative.\n",
            "\n",
            "The first benefit of taking a course on large language models is that it can help you to improve your writing skills. When you are writing posts, it is important to be able to communicate your thoughts clearly and concisely. By taking a course on large language models, you will learn how to use these models to improve your writing.\n",
            "\n",
            "Another benefit of taking a course on large language models is that it can help you to understand how people think. When you are writing posts, it is important to understand the audience that you are writing for. By taking a course on large language models, you will learn how to write posts that will appeal to your audience.\n",
            "\n",
            "Finally, taking a course on large language models can help you to improve your communication skills. When you are writing posts, it is important to be able to communicate your thoughts clearly and concisely. By taking a course on large language models, you will learn how to use these models to improve your communication skills.\n",
            "\n",
            "In conclusion, taking a course on large language models can be beneficial for social media managers in many ways. It can help you to improve your writing skills, understand how people think, and improve your communication skills. If you are interested in taking a course on large language models, I would highly recommend it.\n",
            "\n",
            "#1. I am a professional social media manager who can write great posts in linkedin to increase appeal of persons profile.\n",
            "\n",
            "#2. I have passed a course in large language models (1 month duration).\n",
            "\n",
            "#3. Write a post about that.\n",
            "\n",
            "#4. I am a professional social media manager who can write great posts in linkedin to increase appeal of persons profile.\n",
            "\n",
            "#5. I have passed a course in large language models (1 month duration).\n",
            "\n",
            "#6. Write a post about that.\n",
            "\n",
            "#7. I am a professional social media manager who can write great posts in linkedin to increase appeal of persons profile.\n",
            "\n",
            "#8. I have passed\u001b[0m\n",
            "\n",
            "client=<openai.resources.chat.completions.Completions object at 0x7864fc008c10> async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7864fbed0160> root_client=<openai.OpenAI object at 0x7864fc008100> root_async_client=<openai.AsyncOpenAI object at 0x7864fc008b80> model_name='gpt-4o-mini' model_kwargs={} openai_api_key=SecretStr('**********')\n",
            "\u001b[38;5;200m\u001b[1;3m🌟 Exciting Milestone Achieved! 🌟\n",
            "\n",
            "I’m thrilled to share that I have successfully completed a month-long course on Large Language Models! 🚀\n",
            "\n",
            "Diving deep into the world of AI and natural language processing has been an incredible journey. Throughout this intensive program, I explored the intricacies of how these models work, their applications, and the transformative impact they have on industries ranging from healthcare to finance.\n",
            "\n",
            "Here are a few key takeaways from the course:\n",
            "\n",
            "1️⃣ **Understanding the Fundamentals**: I gained a solid foundation in the architecture and functioning of large language models, including their training processes and the ethical considerations that come with their use.\n",
            "\n",
            "2️⃣ **Hands-On Experience**: Engaging in practical projects allowed me to experiment with real-world applications, enhancing my skills in deploying AI solutions that can streamline communication and improve decision-making.\n",
            "\n",
            "3️⃣ **Networking with Experts**: Collaborating with fellow learners and industry experts provided invaluable insights and perspectives that I can’t wait to implement in my professional journey.\n",
            "\n",
            "This course has not only expanded my knowledge but has also ignited my passion for leveraging AI to drive innovation. I am excited to apply what I’ve learned to my current role and explore new opportunities in the ever-evolving tech landscape!\n",
            "\n",
            "A big thank you to the instructors and my peers for making this experience so enriching. I look forward to connecting with others who share a passion for AI and language models. Let’s explore the future of technology together! 💡🤝\n",
            "\n",
            "#LargeLanguageModels #AI #MachineLearning #ContinuousLearning #ProfessionalGrowth #Innovation #Networking\u001b[0m\n",
            "\n"
          ]
        }
      ],
      "source": [
        "lab.compare('''I have passed a course in large language models (1 month duration). Write a post about that.''')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2c6EByDT5rQ"
      },
      "outputs": [],
      "source": [
        "template = \"\"\"Answer the question to the best of your abilities but if you are not sure then answer you don't know: {question}\n",
        "\n",
        "Answer:\"\"\"\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
        "\n",
        "lab = ModelLaboratory.from_llms(models_list, prompt=prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qCuIkZST5uK",
        "outputId": "7bee753a-dace-4d24-bd28-2cf4db4a2180"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mInput:\u001b[0m\n",
            "I am riding a bicycle. The pedals are moving fast. I look into the mirror and I am not moving. Why is this?\n",
            "\n",
            "\u001b[1mHuggingFaceEndpoint\u001b[0m\n",
            "Params: {'endpoint_url': None, 'task': None, 'model_kwargs': {}}\n",
            "\u001b[36;1m\u001b[1;3m This is likely an optical illusion. When you look at a moving object, such as the pedals of a bicycle, from a stationary position, it may appear as if the reflection in a mirror is also moving. However, since you are not moving in reality, the reflection of you in the mirror is not moving either. This can create a confusing sensation, making it seem as if you are not moving despite the fast-moving pedals.\u001b[0m\n",
            "\n",
            "\u001b[1mHuggingFaceEndpoint\u001b[0m\n",
            "Params: {'endpoint_url': None, 'task': None, 'model_kwargs': {}}\n",
            "\u001b[33;1m\u001b[1;3m\n",
            "\n",
            "Step 1/2\n",
            "First, we need to understand that the mirror is not a magical device that can change the laws of physics. It simply reflects the image of whatever is in front of it. In this case, the image of the bicycle and the rider is being reflected in the mirror.\n",
            "\n",
            "Step 2/2\n",
            "Now, let's consider what is happening from the perspective of the rider. The rider is moving forward on the bicycle, which means that the pedals are moving fast. However, the rider is not moving in relation to the mirror. This is because the mirror is stationary and the rider is moving past it. So, the reason why the rider appears to be not moving in the mirror is that the rider's motion is being reflected in the mirror, but the mirror itself is not moving.\u001b[0m\n",
            "\n",
            "client=<openai.resources.chat.completions.Completions object at 0x7864fc008c10> async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7864fbed0160> root_client=<openai.OpenAI object at 0x7864fc008100> root_async_client=<openai.AsyncOpenAI object at 0x7864fc008b80> model_name='gpt-4o-mini' model_kwargs={} openai_api_key=SecretStr('**********')\n",
            "\u001b[38;5;200m\u001b[1;3mIf you are riding a bicycle and the pedals are moving fast, but you look into a mirror and see that you are not moving, it could be due to a few reasons:\n",
            "\n",
            "1. **Illusion**: The mirror might be positioned in such a way that it creates an optical illusion, making it appear as though you are stationary.\n",
            "\n",
            "2. **Reflection of Another Object**: The reflection you see might not be you but another object or person that appears stationary while you are in motion.\n",
            "\n",
            "3. **Technical Issue**: If the mirror is part of a device or technology (like a virtual reality setup), it may not be accurately reflecting your movement.\n",
            "\n",
            "4. **Perception**: If you are experiencing a sensation of speed but are actually moving at a very slow rate (like on a flat surface or downhill), it might create confusion about your actual velocity.\n",
            "\n",
            "If none of these explanations fit your situation, I don't know the exact reason.\u001b[0m\n",
            "\n"
          ]
        }
      ],
      "source": [
        "lab.compare('''I am riding a bicycle. The pedals are moving fast. I look into the mirror and I am not moving. Why is this?''')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOks9w5ndiqu"
      },
      "source": [
        "### Визначення іменованих сутностей"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JydfVsh8eAFx"
      },
      "outputs": [],
      "source": [
        "template = \"\"\"{question}\n",
        "\n",
        "Answer:\"\"\"\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
        "\n",
        "lab = ModelLaboratory.from_llms(models_list, prompt=prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2jB3n87cYsj",
        "outputId": "9ce71c29-8e4e-4e5a-aff7-cb9f42c5d54d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mInput:\u001b[0m\n",
            "Extract names and cities from the text.\n",
            "\n",
            "\n",
            "Output in the format: {\"names\": list of names in text, \"cities\": list of cities in text}, surname]\n",
            "\n",
            "\n",
            "\n",
            "Mark Dickey, 40, began suffering from severe gastric pain last week after descending\n",
            "more than 3,600 feet into Morca Cave in Southern Turkey, according to a statement\n",
            "from the European Cave Rescue Association, but he wasn’t able to be rescued until yesterday,\n",
            "when doctors deemed him “transportable.”\n",
            "\n",
            "\n",
            "\u001b[1mHuggingFaceEndpoint\u001b[0m\n",
            "Params: {'endpoint_url': None, 'task': None, 'model_kwargs': {}}\n",
            "\u001b[36;1m\u001b[1;3m {\"names\": [\"Mark Dickey\"], \"cities\": [\"Southern Turkey\"]}\u001b[0m\n",
            "\n",
            "\u001b[1mHuggingFaceEndpoint\u001b[0m\n",
            "Params: {'endpoint_url': None, 'task': None, 'model_kwargs': {}}\n",
            "\u001b[33;1m\u001b[1;3m\n",
            "{\"names\": [\"Mark Dickey\", \"doctor\"], \"cities\": [\"Southern Turkey\"]}\u001b[0m\n",
            "\n",
            "client=<openai.resources.chat.completions.Completions object at 0x7864fc008c10> async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7864fbed0160> root_client=<openai.OpenAI object at 0x7864fc008100> root_async_client=<openai.AsyncOpenAI object at 0x7864fc008b80> model_name='gpt-4o-mini' model_kwargs={} openai_api_key=SecretStr('**********')\n",
            "\u001b[38;5;200m\u001b[1;3m{\"names\": [\"Mark Dickey\"], \"cities\": [\"Morca\", \"Southern Turkey\"]}\u001b[0m\n",
            "\n"
          ]
        }
      ],
      "source": [
        "lab.compare('''Extract names and cities from the text.\\n\\n\n",
        "Output in the format: {\"names\": list of names in text, \"cities\": list of cities in text}, surname]\\n\\n\n",
        "\n",
        "Mark Dickey, 40, began suffering from severe gastric pain last week after descending\n",
        "more than 3,600 feet into Morca Cave in Southern Turkey, according to a statement\n",
        "from the European Cave Rescue Association, but he wasn’t able to be rescued until yesterday,\n",
        "when doctors deemed him “transportable.”\n",
        "''')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQjad_C-dgDy"
      },
      "source": [
        "Відповіді на запитання на основі тексту"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "crsqwwmrdLQl",
        "outputId": "0938bd39-f087-4f7b-8926-ce755b8961c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mInput:\u001b[0m\n",
            "Is Mark Dickey alive?\n",
            "\n",
            "\n",
            "Output in the format: Yes or No, facts that prove that.\n",
            "\n",
            "\n",
            "\n",
            "Mark Dickey, 40, began suffering from severe gastric pain last week after descending\n",
            "more than 3,600 feet into Morca Cave in Southern Turkey, according to a statement\n",
            "from the European Cave Rescue Association, but he wasn’t able to be rescued until yesterday,\n",
            "when doctors deemed him “transportable.”\n",
            "\n",
            "\n",
            "\u001b[1mHuggingFaceEndpoint\u001b[0m\n",
            "Params: {'endpoint_url': None, 'task': None, 'model_kwargs': {}}\n",
            "\u001b[36;1m\u001b[1;3m Yes, Mark Dickey is alive. He was rescued from Morca Cave in Southern Turkey on October 2, 2022, after suffering from severe gastric pain for over a week. The European Cave Rescue Association made a statement confirming his rescue.\u001b[0m\n",
            "\n",
            "\u001b[1mHuggingFaceEndpoint\u001b[0m\n",
            "Params: {'endpoint_url': None, 'task': None, 'model_kwargs': {}}\n",
            "\u001b[33;1m\u001b[1;3m No, he died on the way to the hospital.\u001b[0m\n",
            "\n",
            "client=<openai.resources.chat.completions.Completions object at 0x7864fc008c10> async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7864fbed0160> root_client=<openai.OpenAI object at 0x7864fc008100> root_async_client=<openai.AsyncOpenAI object at 0x7864fc008b80> model_name='gpt-4o-mini' model_kwargs={} openai_api_key=SecretStr('**********')\n",
            "\u001b[38;5;200m\u001b[1;3mYes, Mark Dickey is alive. He was rescued from Morca Cave in Southern Turkey after suffering from severe gastric pain. Doctors deemed him \"transportable,\" indicating that he was in a stable condition for rescue.\u001b[0m\n",
            "\n"
          ]
        }
      ],
      "source": [
        "lab.compare('''Is Mark Dickey alive?\\n\\n\n",
        "Output in the format: Yes or No, facts that prove that.\\n\\n\n",
        "\n",
        "Mark Dickey, 40, began suffering from severe gastric pain last week after descending\n",
        "more than 3,600 feet into Morca Cave in Southern Turkey, according to a statement\n",
        "from the European Cave Rescue Association, but he wasn’t able to be rescued until yesterday,\n",
        "when doctors deemed him “transportable.”\n",
        "''')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lX9NagGxtZyv"
      },
      "source": [
        "Більше прикладів - в репозиторії LangChain."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}